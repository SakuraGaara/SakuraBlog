{"meta":{"title":"༺Sakura.Gaara༻","subtitle":null,"description":"梦想总是遥不可及，是不是应该放弃！","author":"༺Sakura.Gaara༻","url":"https://ngames-dev.cn","root":"/"},"pages":[{"title":"分类","date":"2019-09-05T02:54:16.000Z","updated":"2019-09-06T09:23:30.127Z","comments":true,"path":"categories/index.html","permalink":"https://ngames-dev.cn/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2019-09-05T03:04:49.000Z","updated":"2019-09-06T09:23:30.177Z","comments":true,"path":"tags/index.html","permalink":"https://ngames-dev.cn/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Kubernetes架构","slug":"Kubernetes架构","date":"2019-10-30T16:00:00.000Z","updated":"2019-10-31T11:06:27.979Z","comments":true,"path":"2019/10/31/Kubernetes架构/","link":"","permalink":"https://ngames-dev.cn/2019/10/31/Kubernetes架构/","excerpt":"Kubernetes的总架构图","text":"Kubernetes的总架构图 Kubernetes各个组件kube-master[控制节点] master的工作流程图 Kubecfg将特定的请求，比如创建Pod，发送给Kubernetes Client。 Kubernetes Client将请求发送给API server。 API Server根据请求的类型，比如创建Pod时storage类型是pods，然后依此选择何种REST Storage API对请求作出处理。 REST Storage API对的请求作相应的处理。 将处理的结果存入高可用键值存储系统Etcd中。 在API Server响应Kubecfg的请求后，Scheduler会根据Kubernetes Client获取集群中运行Pod及Minion/Node信息。 依据从Kubernetes Client获取的信息，Scheduler将未分发的Pod分发到可用的Minion/Node节点上 API Server[资源操作入口] 提供了资源对象的唯一操作入口，其他所有组件都必须通过它提供的API来操作资源数据，只有API Server与存储通信，其他模块通过API Server访问集群状态。第一，是为了保证集群状态访问的安全。第二，是为了隔离集群状态访问的方式和后端存储实现的方式：API Server是状态访问的方式，不会因为后端存储技术etcd的改变而改变。 作为kubernetes系统的入口，封装了核心对象的增删改查操作，以RESTFul接口方式提供给外部客户和内部组件调用。对相关的资源数据“全量查询”+“变化监听”，实时完成相关的业务功能。 Controller Manager[内部管理控制中心]实现集群故障检测和恢复的自动化工作，负责执行各种控制器，主要有： endpoint-controller：定期关联service和pod(关联信息由endpoint对象维护)，保证service到pod的映射总是最新的。 replication-controller：定期关联replicationController和pod，保证replicationController定义的复制数量与实际运行pod的数量总是一致的。 Scheduler[集群分发调度器] Scheduler收集和分析当前Kubernetes集群中所有Minion节点的资源(内存、CPU)负载情况，然后依此分发新建的Pod到Kubernetes集群中可用的节点。 实时监测Kubernetes集群中未分发和已分发的所有运行的Pod。 Scheduler也监测Minion节点信息，由于会频繁查找Minion节点，Scheduler会缓存一份最新的信息在本地。 最后，Scheduler在分发Pod到指定的Minion节点后，会把Pod相关的信息Binding写回API Server。 kube-node[服务节点] kubelet结构图 Kubelet[节点上的Pod管家] 负责Node节点上pod的创建、修改、监控、删除等全生命周期的管理 定时上报本Node的状态信息给API Server。 kubelet是Master API Server和Minion之间的桥梁，接收Master API Server分配给它的commands和work，与持久性键值存储etcd、file、server和http进行交互，读取配置信息。 具体的工作如下： 设置容器的环境变量、给容器绑定Volume、给容器绑定Port、根据指定的Pod运行一个单一容器、给指定的Pod创建network 容器。 同步Pod的状态、同步Pod的状态、从cAdvisor获取Container info、 pod info、 root info、 machine info。 在容器中运行命令、杀死容器、删除Pod的所有容器。 Proxy[负载均衡、路由转发]Proxy是为了解决外部网络能够访问跨机器集群中容器提供的应用服务而设计的，运行在每个Node上。Proxy提供TCP/UDP sockets的proxy，每创建一种Service，Proxy主要从etcd获取Services和Endpoints的配置信息（也可以从file获取），然后根据配置信息在Minion上启动一个Proxy的进程并监听相应的服务端口，当外部请求发生时，Proxy会根据Load Balancer将请求分发到后端正确的容器处理。 Proxy不但解决了同一主宿机相同服务端口冲突的问题，还提供了Service转发服务端口对外提供服务的能力，Proxy后端使用了随机、轮循负载均衡算法。 kubectl（kubelet client）[集群管理命令行工具集]通过客户端的kubectl命令集操作，API Server响应对应的命令结果，从而达到对kubernetes集群的管理。","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/tags/kubernetes/"}]},{"title":"etcd用户及角色权限","slug":"etcd用户及角色权限","date":"2019-10-09T16:00:00.000Z","updated":"2019-10-14T07:44:27.011Z","comments":true,"path":"2019/10/10/etcd用户及角色权限/","link":"","permalink":"https://ngames-dev.cn/2019/10/10/etcd用户及角色权限/","excerpt":"etcd用户和角色设置：1.etcd默认没有用户2.etcd默认角色guest和root3.etcd默认关闭用户登录认证","text":"etcd用户和角色设置：1.etcd默认没有用户2.etcd默认角色guest和root3.etcd默认关闭用户登录认证 创建root用户etcdctl user add root查看用户etcdctl user list开启/关闭用户登录认证etcdctl auth enable/disable 创建用户etcdctl -u root user add etcduser删除用户etcdctl -u root user remove etcduser用户修改密码etcdctl -u root user passwd etcduser 创建角色etcdctl -u root role add etcdrole查看角色etcdctl -u root role list删除角色etcdctl -u root role remove etcdrole角色授权,角色没有密码，仅仅是定义的一组访问权限,角色的访问权限可以被赋予read（读）,write（写）,readwrite（读和写）权限etcdctl -u root role grant etcdrole –path “/*” –readetcdctl -u root role grant etcdrole –path “/*” –writeetcdctl -u root role grant etcdrole –path “/*” –rw角色权限回收etcdctl -u root role revoke etcdrole –path “/*” –write 用户和角色绑定、解除绑定etcdctl -u root user grant –roles etcdrole etcduseretcdctl -u root user revoke –roles etcdrole etcduser","categories":[{"name":"etcd","slug":"etcd","permalink":"https://ngames-dev.cn/categories/etcd/"}],"tags":[{"name":"etcd","slug":"etcd","permalink":"https://ngames-dev.cn/tags/etcd/"}]},{"title":"二进制k8s安装部署","slug":"二进制k8s安装部署","date":"2019-08-24T16:00:00.000Z","updated":"2019-10-14T07:39:31.008Z","comments":true,"path":"2019/08/25/二进制k8s安装部署/","link":"","permalink":"https://ngames-dev.cn/2019/08/25/二进制k8s安装部署/","excerpt":"所有操作全部用root使用者进行，高可用一般建议大于等于3台的奇数,我们使用3台master来做高可用 k8s各版本组件下载地址: https://github.com/kubernetes/kubernetes/tree/v1.14.3 kubernetes: wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-node-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-client-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-server-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes.tar.gz etcd: wget https://github.com/etcd-io/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz flannel: wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz cni-plugins: wget https://github.com/containernetworking/plugins/releases/download/v0.8.1/cni-plugins-linux-amd64-v0.8.1.tgz docker: wget https://download.docker.com/linux/static/stable/x86_64/docker-18.09.6.tgz cfssl: wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 heapster: wget https://github.com/kubernetes-retired/heapster/archive/v1.5.4.tar.gz","text":"所有操作全部用root使用者进行，高可用一般建议大于等于3台的奇数,我们使用3台master来做高可用 k8s各版本组件下载地址: https://github.com/kubernetes/kubernetes/tree/v1.14.3 kubernetes: wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-node-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-client-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-server-linux-amd64.tar.gz wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes.tar.gz etcd: wget https://github.com/etcd-io/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gz flannel: wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gz cni-plugins: wget https://github.com/containernetworking/plugins/releases/download/v0.8.1/cni-plugins-linux-amd64-v0.8.1.tgz docker: wget https://download.docker.com/linux/static/stable/x86_64/docker-18.09.6.tgz cfssl: wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 heapster: wget https://github.com/kubernetes-retired/heapster/archive/v1.5.4.tar.gz 环境准备 Centos7环境说明master: kube-apiserver,kube-controller-manager,kube-scheduler,flanneIdnode: kubelet,kube-proxy,flanneIdService_CIDR: 10.254.0.0/16 服务网段，部署前路由不可达，部署后集群内部使用IP:Port可达Cluster_CIDR：172.30.0.0/16 pod网段，部署前路由不可达，部署后路由可达(flanneld 保证) hostname IP 部署软件 k8s-master1 192.168.1.31 etcd+keepalived+haproxy+master k8s-master2 192.168.1.32 etcd+keepalived+haproxy+master k8s-master3 192.168.1.33 etcd+keepalived+haproxy+master k8s-worker1 192.168.1.35 docker+node VIP 192.168.1.10 VIP 配置主机环境 /etc/hosts，设置master免密登陆1234192.168.1.31 k8s-master1192.168.1.32 k8s-master2192.168.1.33 k8s-master3 安装前配置 禁止selinux,防火墙,swap分区 123456setenforce 0sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/configsystemctl stop firewalldsystemctl disable firewalldswapoff -a 安装软件包 1yum -y install ntpdate gcc git vim wget 时间统一定时更新 12*/5 * * * * /usr/sbin/ntpdate ntp.api.bz &gt;/dev/null 2&gt;&amp;1 修改文件句柄数 123456789cat &lt;&lt;EOF &gt;&gt;/etc/security/limits.conf* soft nofile 65536* hard nofile 65536* soft nproc 65536* hard nproc 65536* soft memlock unlimited* hard memlock unlimitedEOF ipvs安装 12yum install ipvsadm ipset sysstat conntrack libseccomp -y 开机加载内核模块，并设置开机自动加载 12345678910111213cat &gt; /etc/sysconfig/modules/ipvs.modules &lt;&lt;EOF#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF#然后执行脚本chmod 755 /etc/sysconfig/modules/ipvs.modules &amp;&amp; bash /etc/sysconfig/modules/ipvs.moduleslsmod | grep -e ip_vs -e nf_conntrack_ipv4 修改系统参数 1234567891011121314151617181920212223242526272829303132cat &lt;&lt;EOF &gt; /etc/sysctl.d/k8s.confnet.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 30net.ipv4.tcp_keepalive_probes = 10net.ipv6.conf.all.disable_ipv6 = 1net.ipv6.conf.default.disable_ipv6 = 1net.ipv6.conf.lo.disable_ipv6 = 1net.ipv4.neigh.default.gc_stale_time = 120net.ipv4.conf.all.rp_filter = 0net.ipv4.conf.default.rp_filter = 0net.ipv4.conf.default.arp_announce = 2net.ipv4.conf.lo.arp_announce = 2net.ipv4.conf.all.arp_announce = 2net.ipv4.ip_forward = 1net.ipv4.tcp_max_tw_buckets = 5000net.ipv4.tcp_syncookies = 1net.ipv4.tcp_max_syn_backlog = 1024net.ipv4.tcp_synack_retries = 2net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.netfilter.nf_conntrack_max = 2310720fs.inotify.max_user_watches=89100fs.may_detach_mounts = 1fs.file-max = 52706963fs.nr_open = 52706963net.bridge.bridge-nf-call-arptables = 1vm.swappiness = 0vm.overcommit_memory=1vm.panic_on_oom=0EOFsysctl --system 预留内存,避免由于内存耗尽导致ssh连不上主机,比如100M，资源充足建议大点 123echo 'vm.min_free_kbytes=100000' &gt;&gt; /etc/sysctl.confsysctl -p 部署docker 安装yum源工具包 1yum install -y yum-utils device-mapper-persistent-data lvm2 下载docker-ce官方的yum源配置文件 1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装docker-ce相应版本 1yum -y install docker-ce.x86_64 配置daemon, 因为kubelet的启动环境变量要与docker的cgroup-driver驱动相同 123456789101112131415mkdir -p /etc/docker &amp;&amp; cat &gt; /etc/docker/daemon.json &lt;&lt;EOF&#123; \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": &#123; \"max-size\": \"100m\" &#125;, \"storage-driver\": \"overlay2\", \"storage-opts\": [ \"overlay2.override_kernel_check=true\" ], \"registry-mirrors\": [\"https://uyah70su.mirror.aliyuncs.com\"]&#125;EOF 设置开机自启动 1systemctl restart docker &amp;&amp; systemctl enable docker &amp;&amp; systemctl status docker 部署etcdetcd是用来保存集群所有状态的 Key/Value 存储系统，常用于服务发现、共享配置以及并发控制（如 leader 选举、分布式锁等）。kubernetes 使用 etcd 存储所有运行数据。 所有 Kubernetes 组件会通过 API Server 来跟 Etcd 进行沟通从而保存或读取资源状态。有条件的可以单独几台机器跑,不过需要配置apiserver指向etcd集群。 安装cfssl12345wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O /usr/local/bin/cfssl-certinfochmod +x /usr/local/bin/cfssl* 安装配置etcd 配置etcd证书 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465mkdir /root/ssl &amp;&amp; cd /root/sslcat &gt; ca-config.json &lt;&lt;EOF&#123;\"signing\": &#123;\"default\": &#123; \"expiry\": \"8760h\"&#125;,\"profiles\": &#123; \"kubernetes\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"8760h\" &#125;&#125;&#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123;\"CN\": \"kubernetes\",\"key\": &#123;\"algo\": \"rsa\",\"size\": 2048&#125;,\"names\": [&#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"k8s\", \"OU\": \"System\"&#125;]&#125;EOFcat &gt; etcd-csr.json &lt;&lt;EOF&#123; \"CN\": \"etcd\", \"hosts\": [ \"127.0.0.1\", \"192.168.1.31\", \"192.168.1.32\", \"192.168.1.33\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF 创建etcd证书 1234567891011121314cfssl gencert -initca ca-csr.json | cfssljson -bare cacfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes etcd-csr.json | cfssljson -bare etcd#生产后证书包含文件如下，共9个ca-config.jsonca.csrca-csr.jsonca-key.pemca.pemetcd.csretcd-csr.jsonetcd-key.pemetcd.pem 将生成好的etcd.pem和etcd-key.pem以及ca.pem三个文件拷贝到etcd机器上 123456mkdir -p /etc/kubernetes/ssl &amp;&amp; cp *.pem /etc/kubernetes/ssl/ssh -n 192.168.1.32 \"mkdir -p /etc/kubernetes/ssl &amp;&amp; exit\"ssh -n 192.168.1.33 \"mkdir -p /etc/kubernetes/ssl &amp;&amp; exit\"scp -r /etc/kubernetes/ssl/*.pem 192.168.1.32:/etc/kubernetes/ssl/scp -r /etc/kubernetes/ssl/*.pem 192.168.1.33:/etc/kubernetes/ssl/ 配置部署etcd 12345wget https://github.com/etcd-io/etcd/releases/download/v3.3.13/etcd-v3.3.13-linux-amd64.tar.gztar -zxvf etcd-v3.3.13-linux-amd64.tar.gzcp etcd-v3.3.13-linux-amd64/etcd* /usr/local/binscp etcd-v3.3.13-linux-amd64/etcd* 192.168.1.32:/usr/local/binscp etcd-v3.3.13-linux-amd64/etcd* 192.168.1.33:/usr/local/bin 创建启动配置文件(三台配置文件不同) k8s-master1: 1234567891011121314151617181920212223242526272829303132333435363738cat &lt;&lt;EOF &gt;/etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ExecStart=/usr/local/bin/etcd \\ --name k8s-master1 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls https://192.168.1.31:2380 \\ --listen-peer-urls https://192.168.1.31:2380 \\ --listen-client-urls https://192.168.1.31:2379,http://127.0.0.1:2379 \\ --advertise-client-urls https://192.168.1.31:2379 \\ --initial-cluster-token etcd-cluster-0 \\ --initial-cluster k8s-master1=https://192.168.1.31:2380,k8s-master2=https://192.168.1.32:2380,k8s-master3=https://192.168.1.33:2380 \\ --initial-cluster-state new \\ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF#启动etcd服务mkdir /var/lib/etcdsystemctl daemon-reload &amp;&amp; systemctl enable etcd.service &amp;&amp; systemctl start etcd.service &amp;&amp; systemctl status etcd k8s-master2: 1234567891011121314151617181920212223242526272829303132333435363738cat &lt;&lt;EOF &gt;/etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ExecStart=/usr/local/bin/etcd \\ --name k8s-master2 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls https://192.168.1.32:2380 \\ --listen-peer-urls https://192.168.1.32:2380 \\ --listen-client-urls https://192.168.1.32:2379,http://127.0.0.1:2379 \\ --advertise-client-urls https://192.168.1.32:2379 \\ --initial-cluster-token etcd-cluster-0 \\ --initial-cluster k8s-master1=https://192.168.1.31:2380,k8s-master2=https://192.168.1.32:2380,k8s-master3=https://192.168.1.33:2380 \\ --initial-cluster-state new \\ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF#启动etcd服务mkdir /var/lib/etcdsystemctl daemon-reload &amp;&amp; systemctl enable etcd.service &amp;&amp; systemctl start etcd.service &amp;&amp; systemctl status etcd k8s-master3: 1234567891011121314151617181920212223242526272829303132333435363738cat &lt;&lt;EOF &gt;/etc/systemd/system/etcd.service[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/ExecStart=/usr/local/bin/etcd \\ --name k8s-master3 \\ --cert-file=/etc/kubernetes/ssl/etcd.pem \\ --key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --peer-cert-file=/etc/kubernetes/ssl/etcd.pem \\ --peer-key-file=/etc/kubernetes/ssl/etcd-key.pem \\ --peer-trusted-ca-file=/etc/kubernetes/ssl/ca.pem \\ --initial-advertise-peer-urls https://192.168.1.33:2380 \\ --listen-peer-urls https://192.168.1.33:2380 \\ --listen-client-urls https://192.168.1.33:2379,http://127.0.0.1:2379 \\ --advertise-client-urls https://192.168.1.33:2379 \\ --initial-cluster-token etcd-cluster-0 \\ --initial-cluster k8s-master1=https://192.168.1.31:2380,k8s-master2=https://192.168.1.32:2380,k8s-master3=https://192.168.1.33:2380 \\ --initial-cluster-state new \\ --data-dir=/var/lib/etcdRestart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF#启动etcd服务mkdir /var/lib/etcdsystemctl daemon-reload &amp;&amp; systemctl enable etcd.service &amp;&amp; systemctl start etcd.service &amp;&amp; systemctl status etcd 验证集群123456etcdctl --ca-file=/etc/kubernetes/ssl/ca.pem --cert-file=/etc/kubernetes/ssl/etcd.pem --key-file=/etc/kubernetes/ssl/etcd-key.pem cluster-health#返回如下正常member 22a9d61e6821c4d is healthy: got healthy result from https://192.168.1.32:2379member 68afffba56612fd is healthy: got healthy result from https://192.168.1.31:2379member ff1f72bab5edb59f is healthy: got healthy result from https://192.168.1.33:2379cluster is healthy 部署flannel所有的节点都需要安装flannel，，主要目的是跨主机的docker能够互相通信，也是保障kubernetes集群的网络基础和保障 生产TLS证书，是让kubectl当做client证书使用,(证书只需要生成一次) 1234567891011121314151617181920cd /root/sslcat &gt; flanneld-csr.json &lt;&lt;EOF&#123; \"CN\": \"flanneld\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF 生成证书和私钥 1234567891011cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes flanneld-csr.json | cfssljson -bare flanneld#包含以下文件flanneld.csrflanneld-csr.jsonflanneld-key.pemflanneld.pem#然后将证书拷贝到所有节点下cp flanneld*.pem /etc/kubernetes/sslscp flanneld*.pem 192.168.1.32:/etc/kubernetes/sslscp flanneld*.pem 192.168.1.33:/etc/kubernetes/ssl 安装flannel 123456wget https://github.com/coreos/flannel/releases/download/v0.11.0/flannel-v0.11.0-linux-amd64.tar.gztar -zvxf flannel-v0.11.0-linux-amd64.tar.gzcp flanneld mk-docker-opts.sh /usr/local/bincp flanneld mk-docker-opts.sh /usr/local/binscp flanneld mk-docker-opts.sh 192.168.1.32:/usr/local/binscp flanneld mk-docker-opts.sh 192.168.1.33:/usr/local/bin 向etcd写入集群Pod网段信息，在etcd集群中任意一台执行一次即可 123456789101112131415161718192021222324252627etcdctl \\--endpoints=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\--ca-file=/etc/kubernetes/ssl/ca.pem \\--cert-file=/etc/kubernetes/ssl/flanneld.pem \\--key-file=/etc/kubernetes/ssl/flanneld-key.pem \\mk /kubernetes/network/config '&#123;\"Network\":\"172.30.0.0/16\", \"SubnetLen\": 24, \"Backend\": &#123;\"Type\": \"vxlan\"&#125;&#125;'#返回结果&#123;\"Network\":\"172.30.0.0/16\", \"SubnetLen\": 24, \"Backend\": &#123;\"Type\": \"vxlan\"&#125;#验证#列出键值存储的目录etcdctl \\--ca-file=/etc/kubernetes/ssl/ca.pem \\--cert-file=/etc/kubernetes/ssl/flanneld.pem \\--key-file=/etc/kubernetes/ssl/flanneld-key.pem ls -r#查看键值存储etcdctl \\--ca-file=/etc/kubernetes/ssl/ca.pem \\--cert-file=/etc/kubernetes/ssl/flanneld.pem \\--key-file=/etc/kubernetes/ssl/flanneld-key.pem get /kubernetes/network/config#查看已分配pod的子网列表（暂时没有为docker分配子网，启动flannel可以查看）etcdctl \\--ca-file=/etc/kubernetes/ssl/ca.pem \\--cert-file=/etc/kubernetes/ssl/flanneld.pem \\--key-file=/etc/kubernetes/ssl/flanneld-key.pem ls /kubernetes/network/subnets 创建flannel.service文件 123456789101112131415161718192021222324cat &gt; /etc/systemd/system/flannel.service &lt;&lt; EOF[Unit]Description=Flanneld overlay address etcd agentAfter=network.targetAfter=network-online.targetWants=network-online.targetAfter=etcd.serviceBefore=docker.service[Service]Type=notifyExecStart=/usr/local/bin/flanneld \\ -etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ -etcd-certfile=/etc/kubernetes/ssl/flanneld.pem \\ -etcd-keyfile=/etc/kubernetes/ssl/flanneld-key.pem \\ -etcd-endpoints=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\ -etcd-prefix=/kubernetes/networkExecStartPost=/usr/local/bin/mk-docker-opts.sh -k DOCKER_NETWORK_OPTIONS -d /run/flannel/dockerRestart=on-failure[Install]WantedBy=multi-user.targetRequiredBy=docker.serviceEOF 启动flannel 1systemctl daemon-reload &amp;&amp; systemctl enable flannel &amp;&amp; systemctl start flannel &amp;&amp; systemctl status flannel 验证flannel 12345678910111213141516cat /run/flannel/docker#/run/flannel/docker是flannel分配给docker的子网信息，显示如下DOCKER_OPT_BIP=\"--bip=172.30.10.1/24\"DOCKER_OPT_IPMASQ=\"--ip-masq=true\"DOCKER_OPT_MTU=\"--mtu=1450\"DOCKER_NETWORK_OPTIONS=\" --bip=172.30.10.1/24 --ip-masq=true --mtu=1450\"ip add | grep flannel 4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default inet 172.30.10.0/32 scope global flannel.1cat /run/flannel/subnet.envFLANNEL_NETWORK=172.30.0.0/16FLANNEL_SUBNET=172.30.10.1/24FLANNEL_MTU=1450FLANNEL_IPMASQ=false 配置docker支持flannel 123vim /etc/systemd/system/multi-user.target.wants/docker.serviceEnvironmentFile=/run/flannel/dockerExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sock $DOCKER_NETWORK_OPTIONS 重启docker,然后可以查看到已分配pod的子网列表 12345systemctl daemon-reload &amp;&amp; systemctl restart docker &amp;&amp; systemctl status dockerip add | grep docker3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN group default inet 172.30.10.1/24 brd 172.30.10.255 scope global docker0 设置CNI插件支持flannel 1234567891011121314151617181920212223wget https://github.com/containernetworking/plugins/releases/download/v0.8.1/cni-plugins-linux-amd64-v0.8.1.tgzmkdir /opt/cnitar -zxvf cni-plugins-linux-amd64-v0.8.1.tgz -C /opt/cnimkdir -p /etc/cni/net.dcat &gt; /etc/cni/net.d/10-default.conf &lt;&lt;EOF&#123; \"name\": \"flannel\", \"type\": \"flannel\", \"delegate\": &#123; \"bridge\": \"docker0\", \"isDefaultGateway\": true, \"mtu\": 1400 &#125;&#125;EOFcp /opt/cni/* /usr/local/binscp /opt/cni/* 192.168.1.32:/usr/local/binscp /opt/cni/* 192.168.1.33:/usr/local/binssh -n 192.168.1.32 \"mkdir -p /etc/cni/net.d &amp;&amp; exit\"ssh -n 192.168.1.33 \"mkdir -p /etc/cni/net.d &amp;&amp; exit\"scp /etc/cni/net.d/10-default.conf 192.168.1.32:/etc/cni/net.d/scp /etc/cni/net.d/10-default.conf 192.168.1.33:/etc/cni/net.d/ 部署keepalived+haproxykeepalived 提供 kube-apiserver 对外服务的 VIP；haproxy 监听 VIP，后端连接所有 kube-apiserver 实例，提供健康检查和负载均衡功能 本文档复用 master 节点的三台机器，haproxy 监听的端口(8443) 需要与 kube-apiserver 的端口 6443 不同，避免冲突。 keepalived 在运行过程中周期检查本机的 haproxy 进程状态，如果检测到 haproxy 进程异常，则触发重新选主的过程，VIP 将飘移到新选出来的主节点，从而实现 VIP 的高可用。所有组件（如 kubeclt、apiserver、controller-manager、scheduler 等）都通过 VIP 和 haproxy 监听的 8443 端口访问 kube-apiserver 服务。 部署haproxy 安装配置haproxy 123456789101112131415161718192021222324252627282930313233343536373839404142yum install -y haproxycat &lt;&lt; EOF &gt; /etc/haproxy/haproxy.cfgglobal log 127.0.0.1 local2 chroot /var/lib/haproxy pidfile /var/run/haproxy.pid maxconn 4000 user haproxy group haproxy daemondefaults mode tcp log global retries 3 timeout connect 10s timeout client 1m timeout server 1mlisten admin_stats bind 0.0.0.0:9090 mode http log 127.0.0.1 local0 err stats refresh 30s stats uri /status stats realm welcome login\\ Haproxy stats auth admin:123456 stats hide-version stats admin if TRUEfrontend kubernetes bind *:8443 mode tcp default_backend kubernetes-masterbackend kubernetes-master balance roundrobin server k8s-master1 192.168.1.31:6443 check maxconn 2000 server k8s-master2 192.168.1.32:6443 check maxconn 2000 server k8s-master3 192.168.1.33:6443 check maxconn 2000EOF 启动haproxy 1systemctl enable haproxy &amp;&amp; systemctl start haproxy &amp;&amp; systemctl status haproxy 部署keepalived 安装keepalived 1yum install -y keepalived keepalived配置文件，注意网卡interface未必全部一样，配置VIP为192.168.1.10 k8s-master1: 1234567891011121314151617181920212223242526272829303132333435363738cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.confglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; script \"curl -k https://192.168.1.10:8443\" interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens160 virtual_router_id 100 priority 100 advert_int 1 mcast_src_ip 192.168.1.31 nopreempt authentication &#123; auth_type PASS auth_pass fana123 &#125; unicast_peer &#123; 192.168.1.32 192.168.1.33 &#125; virtual_ipaddress &#123; 192.168.1.10/24 &#125; track_script &#123; CheckK8sMaster &#125;&#125;EOF k8s-master2: 12345678910111213141516171819202122232425262728293031323334353637cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.confglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; script \"curl -k https://192.168.1.10:8443\" interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens32 virtual_router_id 100 priority 90 advert_int 1 mcast_src_ip 192.168.1.32 nopreempt authentication &#123; auth_type PASS auth_pass fana123 &#125; unicast_peer &#123; 192.168.1.31 192.168.1.33 &#125; virtual_ipaddress &#123; 192.168.1.10/24 &#125; track_script &#123; CheckK8sMaster &#125;&#125;EOF k8s-master3: 1234567891011121314151617181920212223242526272829303132333435363738cat &lt;&lt;EOF &gt; /etc/keepalived/keepalived.confglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; script \"curl -k https://192.168.1.10:8443\" interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens160 virtual_router_id 100 priority 80 advert_int 1 mcast_src_ip 192.168.1.33 nopreempt authentication &#123; auth_type PASS auth_pass fana123 &#125; unicast_peer &#123; 192.168.1.31 192.168.1.32 &#125; virtual_ipaddress &#123; 192.168.1.10/24 &#125; track_script &#123; CheckK8sMaster &#125;&#125;EOF 启动keepalived 1systemctl restart keepalived &amp;&amp; systemctl enable keepalived &amp;&amp; systemctl status keepalived 查看三台vip(只有一台为VIP) 12ip addr |grep 1.10 inet 192.168.1.10/24 scope global secondary ens160 部署masterkube-scheduler，kube-controller-manager 和 kube-apiserver 三者的功能紧密相关；同时kube-scheduler 和 kube-controller-manager 只能有一个进程处于工作状态，如果运行多个，则需要通过选举产生一个 leader； 部署kubectl命令工具 创建CA证书 123456wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-server-linux-amd64.tar.gztar -zxvf kubernetes-server-linux-amd64.tar.gzcd kubernetes/server/bincp kube-apiserver kubeadm kube-controller-manager kubectl kube-scheduler /usr/local/binscp -r kube-apiserver kubeadm kube-controller-manager kubectl kube-scheduler k8s-master2:/usr/local/binscp -r kube-apiserver kubeadm kube-controller-manager kubectl kube-scheduler k8s-master3:/usr/local/bin 创建CA证书 1234567891011121314151617181920cd /root/sslcat &gt; admin-csr.json &lt;&lt;EOF&#123; \"CN\": \"admin\", \"hosts\": [], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"system:masters\", \"OU\": \"System\" &#125; ]&#125;EOF 生成证书和私钥 1234cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes admin-csr.json | cfssljson -bare admin 创建($HOME)/.kube/config文件 12345kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.10:8443 \\ --kubeconfig=kubectl.kubeconfig 设置客户端认证参数 12345kubectl config set-credentials admin \\ --client-certificate=admin.pem \\ --client-key=admin-key.pem \\ --embed-certs=true \\ --kubeconfig=kubectl.kubeconfig 设置上下文参数 1234kubectl config set-context kubernetes \\ --cluster=kubernetes \\ --user=admin \\ --kubeconfig=kubectl.kubeconfig 设置默认上下文 1kubectl config use-context kubernetes --kubeconfig=kubectl.kubeconfig 拷贝kubectl.kubeconfig文件 123456789cp kubectl.kubeconfig ~/.kube/configssh -n 192.168.1.32 \"mkdir -p /root/.kube &amp;&amp; exit\"ssh -n 192.168.1.33 \"mkdir -p /root/.kube &amp;&amp; exit\"scp kubectl.kubeconfig 192.168.1.32:/root/.kube/configscp kubectl.kubeconfig 192.168.1.33:/root/.kube/configcp admin*.pem /etc/kubernetes/ssl/scp admin*.pem 192.168.1.32:/etc/kubernetes/ssl/scp admin*.pem 192.168.1.33:/etc/kubernetes/ssl/ 部署api-server 创建CA证书,hosts字段指定授权使用该证书的IP或域名列表，这里列出了VIP/apiserver节点IP/kubernetes服务IP和域名 1234567891011121314151617181920212223242526272829303132cd /root/sslcat &gt; kubernetes-csr.json &lt;&lt;EOF&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"192.168.1.31\", \"192.168.1.32\", \"192.168.1.33\", \"192.168.1.10\", \"10.254.0.1\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF 生成证书和私钥 1234cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 将证书拷贝到其他master节点 123cp kubernetes*.pem /etc/kubernetes/ssl/scp kubernetes*.pem 192.168.1.32:/etc/kubernetes/ssl/scp kubernetes*.pem 192.168.1.33:/etc/kubernetes/ssl/ 创建加密配置文件，创建kube-apiserver使用的客户端令牌文件 1234567891011121314151617cat &gt; encryption-config.yaml &lt;&lt;EOFkind: EncryptionConfigapiVersion: v1resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: $(head -c 32 /dev/urandom | base64) - identity: &#123;&#125;EOFcat &lt;&lt;EOF &gt; bootstrap-token.csv$(head -c 32 /dev/urandom | base64),kubelet-bootstrap,10001,\"system:kubelet-bootstrap\"EOF 将加密文件拷贝到其他master节点 123cp encryption-config.yaml bootstrap-token.csv /etc/kubernetes/sslscp encryption-config.yaml bootstrap-token.csv 192.168.1.32:/etc/kubernetes/sslscp encryption-config.yaml bootstrap-token.csv 192.168.1.33:/etc/kubernetes/ssl 创建kube-apiserver.service文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758cat &gt; /etc/systemd/system/kube-apiserver.service &lt;&lt; EOF[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]ExecStart=/usr/local/bin/kube-apiserver \\ --enable-admission-plugins=NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\ --anonymous-auth=false \\ --experimental-encryption-provider-config=/etc/kubernetes/ssl/encryption-config.yaml \\ --advertise-address=0.0.0.0 \\ --bind-address=0.0.0.0 \\ --insecure-bind-address=127.0.0.1 \\ --secure-port=6443 \\ --insecure-port=0 \\ --authorization-mode=Node,RBAC \\ --runtime-config=api/all \\ --enable-bootstrap-token-auth \\ --service-cluster-ip-range=10.254.0.0/16 \\ --service-node-port-range=30000-32700 \\ --tls-cert-file=/etc/kubernetes/ssl/kubernetes.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kubernetes-key.pem \\ --client-ca-file=/etc/kubernetes/ssl/ca.pem \\ --kubelet-client-certificate=/etc/kubernetes/ssl/kubernetes.pem \\ --kubelet-client-key=/etc/kubernetes/ssl/kubernetes-key.pem \\ --service-account-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --etcd-cafile=/etc/kubernetes/ssl/ca.pem \\ --etcd-certfile=/etc/kubernetes/ssl/kubernetes.pem \\ --etcd-keyfile=/etc/kubernetes/ssl/kubernetes-key.pem \\ --etcd-servers=https://192.168.1.31:2379,https://192.168.1.32:2379,https://192.168.1.33:2379 \\ --enable-swagger-ui=true \\ --allow-privileged=true \\ --apiserver-count=3 \\ --audit-log-maxage=30 \\ --audit-log-maxbackup=3 \\ --audit-log-maxsize=100 \\ --audit-log-path=/var/log/kubernetes/kube-apiserver-audit.log \\ --event-ttl=1h \\ --alsologtostderr=true \\ --logtostderr=false \\ --log-dir=/var/log/kubernetes \\ --v=2Restart=on-failureRestartSec=5Type=notifyLimitNOFILE=65536[Install]WantedBy=multi-user.targetEOFmkdir -p /var/log/kubernetesssh -n 192.168.1.32 \"mkdir -p /var/log/kubernetes &amp;&amp; exit\"ssh -n 192.168.1.33 \"mkdir -p /var/log/kubernetes &amp;&amp; exit\"scp /etc/systemd/system/kube-apiserver.service 192.168.1.32:/etc/systemd/system/scp /etc/systemd/system/kube-apiserver.service 192.168.1.33:/etc/systemd/system/# --bind-address --insecure-bind-address 填固定IPv4地址，不然启动为ipv6，controller-manager总是报错 启动服务 1systemctl daemon-reload &amp;&amp; systemctl enable kube-apiserver &amp;&amp; systemctl start kube-apiserver &amp;&amp; systemctl status kube-apiserver 授予kubernetes证书访问kubelet api权限。在执行kubectl exec、run、logs 等命令时，apiserver会转发到kubelet。这里定义 RBAC规则，授权apiserver调用kubelet API。 123kubectl create clusterrolebinding kube-apiserver:kubelet-apis --clusterrole=system:kubelet-api-admin --user kubernetes＃预定义的ClusterRole system:kubelet-api-admin授予访问kubelet所有 API 的权限kubectl describe clusterrole system:kubelet-api-admin 检查api-server和集群状态 123456789101112131415161718netstat -tnlp|grep 6443tcp6 0 0 :::6443 :::* LISTEN 23462/kube-apiserve kubectl cluster-infoKubernetes master is running at https://192.168.1.10:8443To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.kubectl get all --all-namespacesNAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEdefault service/kubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 6m44skubectl get componentstatusesNAME STATUS MESSAGE ERRORscheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused controller-manager Unhealthy Get http://127.0.0.1:10252/healthz: dial tcp 127.0.0.1:10252: connect: connection refused etcd-0 Healthy &#123;\"health\":\"true\"&#125; etcd-1 Healthy &#123;\"health\":\"true\"&#125; etcd-2 Healthy &#123;\"health\":\"true\"&#125; 部署kube-controller-manager该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性。 创建CA证书 12345678910111213141516171819202122232425cd /root/sslcat &gt; kube-controller-manager-csr.json &lt;&lt; EOF&#123; \"CN\": \"system:kube-controller-manager\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"hosts\": [ \"127.0.0.1\", \"192.168.1.31\", \"192.168.1.32\", \"192.168.1.33\" ], \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"system:kube-controller-manager\", \"OU\": \"System\" &#125; ]&#125;EOF 生成证书 1234cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kube-controller-manager-csr.json | cfssljson -bare kube-controller-manager 将证书拷贝到其他master节点 123cp kube-controller-manager*.pem /etc/kubernetes/ssl/scp kube-controller-manager*.pem 192.168.1.32:/etc/kubernetes/ssl/scp kube-controller-manager*.pem 192.168.1.33:/etc/kubernetes/ssl/ 创建kubeconfig文件 123456789101112131415161718kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.10:8443 \\ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-credentials system:kube-controller-manager \\ --client-certificate=kube-controller-manager.pem \\ --client-key=kube-controller-manager-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-controller-manager.kubeconfigkubectl config set-context system:kube-controller-manager \\ --cluster=kubernetes \\ --user=system:kube-controller-manager \\ --kubeconfig=kube-controller-manager.kubeconfigkubectl config use-context system:kube-controller-manager --kubeconfig=kube-controller-manager.kubeconfig 拷贝kube-controller-manager.kubeconfig到其他master节点 123cp kube-controller-manager.kubeconfig /etc/kubernetes/ssl/scp kube-controller-manager.kubeconfig 192.168.1.32:/etc/kubernetes/ssl/scp kube-controller-manager.kubeconfig 192.168.1.33:/etc/kubernetes/ssl/ 创建kube-controller-manager.service文件 123456789101112131415161718192021222324252627282930313233343536373839cat &gt; /etc/systemd/system/kube-controller-manager.service &lt;&lt; EOF[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-controller-manager \\ --address=127.0.0.1 \\ --master=https://192.168.1.10:8443 \\ --kubeconfig=/etc/kubernetes/ssl/kube-controller-manager.kubeconfig \\ --allocate-node-cidrs=true \\ --authentication-kubeconfig=/etc/kubernetes/ssl/kube-controller-manager.kubeconfig \\ --service-cluster-ip-range=10.254.0.0/16 \\ --cluster-cidr=172.30.0.0/16 \\ --cluster-name=kubernetes \\ --cluster-signing-cert-file=/etc/kubernetes/ssl/ca.pem \\ --cluster-signing-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --experimental-cluster-signing-duration=8760h \\ --leader-elect=true \\ --feature-gates=RotateKubeletServerCertificate=true \\ --controllers=*,bootstrapsigner,tokencleaner \\ --horizontal-pod-autoscaler-use-rest-clients=true \\ --horizontal-pod-autoscaler-sync-period=10s \\ --tls-cert-file=/etc/kubernetes/ssl/kube-controller-manager.pem \\ --tls-private-key-file=/etc/kubernetes/ssl/kube-controller-manager-key.pem \\ --service-account-private-key-file=/etc/kubernetes/ssl/ca-key.pem \\ --root-ca-file=/etc/kubernetes/ssl/ca.pem \\ --use-service-account-credentials=true \\ --alsologtostderr=true \\ --logtostderr=false \\ --log-dir=/var/log/kubernetes \\ --v=2Restart=onRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF 拷贝到其他master节点，然后启动服务 1234scp /etc/systemd/system/kube-controller-manager.service 192.168.1.32:/etc/systemd/system/scp /etc/systemd/system/kube-controller-manager.service 192.168.1.33:/etc/systemd/system/systemctl daemon-reload &amp;&amp; systemctl enable kube-controller-manager &amp;&amp; systemctl start kube-controller-manager &amp;&amp; systemctl status kube-controller-manager 检查服务 12345678910111213141516171819202122232425netstat -tnlp|grep kube-controlltcp 0 0 127.0.0.1:10252 0.0.0.0:* LISTEN 24125/kube-controll tcp6 0 0 :::10257 :::* LISTEN 24125/kube-controllkubectl get csNAME STATUS MESSAGE ERRORscheduler Unhealthy Get http://127.0.0.1:10251/healthz: dial tcp 127.0.0.1:10251: connect: connection refused controller-manager Healthy ok etcd-0 Healthy &#123;\"health\":\"true\"&#125; etcd-1 Healthy &#123;\"health\":\"true\"&#125; etcd-2 Healthy &#123;\"health\":\"true\"&#125;检查leader所在机器,如下k8s-master1选为leaderkubectl get endpoints kube-controller-manager --namespace=kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;\"holderIdentity\":\"k8s-master1_836ddc44-c586-11e9-94e1-000c29178d85\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2019-08-23T09:15:13Z\",\"renewTime\":\"2019-08-23T09:16:49Z\",\"leaderTransitions\":0&#125;' creationTimestamp: \"2019-08-23T09:15:13Z\" name: kube-controller-manager namespace: kube-system resourceVersion: \"654\" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager uid: 8371cd23-c586-11e9-b643-000c29327412 部署kube-scheduler该集群包含 3 个节点，启动后将通过竞争选举机制产生一个 leader 节点，其它节点为阻塞状态。当 leader 节点不可用后，剩余节点将再次进行选举产生新的 leader 节点，从而保证服务的可用性 创建CA证书 12345678910111213141516171819202122232425cd /root/sslcat &gt; kube-scheduler-csr.json &lt;&lt; EOF&#123; \"CN\": \"system:kube-scheduler\", \"hosts\": [ \"127.0.0.1\", \"192.168.1.31\", \"192.168.1.32\", \"192.168.1.33\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"system:kube-scheduler\", \"OU\": \"System\" &#125; ]&#125;EOF 生成证书 1234cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kube-scheduler-csr.json | cfssljson -bare kube-scheduler 创建kube-scheduler.kubeconfig文件 123456789101112131415161718kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.10:8443 \\ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-credentials system:kube-scheduler \\ --client-certificate=kube-scheduler.pem \\ --client-key=kube-scheduler-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-scheduler.kubeconfigkubectl config set-context system:kube-scheduler \\ --cluster=kubernetes \\ --user=system:kube-scheduler \\ --kubeconfig=kube-scheduler.kubeconfigkubectl config use-context system:kube-scheduler --kubeconfig=kube-scheduler.kubeconfig 拷贝kubeconfig到其他master节点 123cp kube-scheduler.kubeconfig kube-scheduler*.pem /etc/kubernetes/ssl/scp kube-scheduler.kubeconfig kube-scheduler*.pem 192.168.1.32:/etc/kubernetes/ssl/scp kube-scheduler.kubeconfig kube-scheduler*.pem 192.168.1.33:/etc/kubernetes/ssl/ 创建kube-scheduler.service文件 123456789101112131415161718192021cat &gt; /etc/systemd/system/kube-scheduler.service &lt;&lt; EOF[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/GoogleCloudPlatform/kubernetes[Service]ExecStart=/usr/local/bin/kube-scheduler \\ --address=127.0.0.1 \\ --master=https://192.168.1.10:8443 \\ --kubeconfig=/etc/kubernetes/ssl/kube-scheduler.kubeconfig \\ --leader-elect=true \\ --alsologtostderr=true \\ --logtostderr=false \\ --log-dir=/var/log/kubernetes \\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF 将kube-scheduler.service拷贝到其他master节点，然后启动服务 1234scp /etc/systemd/system/kube-scheduler.service 192.168.1.32:/etc/systemd/systemscp /etc/systemd/system/kube-scheduler.service 192.168.1.33:/etc/systemd/systemsystemctl daemon-reload &amp;&amp; systemctl enable kube-scheduler &amp;&amp; systemctl start kube-scheduler &amp;&amp; systemctl status kube-scheduler 检查服务 123456789101112131415161718192021222324netstat -lnpt|grep kube-schedtcp 0 0 127.0.0.1:10251 0.0.0.0:* LISTEN 24760/kube-schedule tcp6 0 0 :::10259 :::* LISTEN 24760/kube-schedule kubectl get csNAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;\"health\":\"true\"&#125; etcd-0 Healthy &#123;\"health\":\"true\"&#125; etcd-1 Healthy &#123;\"health\":\"true\"&#125; kubectl get endpoints kube-scheduler --namespace=kube-system -o yamlapiVersion: v1kind: Endpointsmetadata: annotations: control-plane.alpha.kubernetes.io/leader: '&#123;\"holderIdentity\":\"k8s-master3_74560974-c588-11e9-b994-000c297ea248\",\"leaseDurationSeconds\":15,\"acquireTime\":\"2019-08-23T09:29:07Z\",\"renewTime\":\"2019-08-23T09:30:38Z\",\"leaderTransitions\":0&#125;' creationTimestamp: \"2019-08-23T09:29:07Z\" name: kube-scheduler namespace: kube-system resourceVersion: \"1365\" selfLink: /api/v1/namespaces/kube-system/endpoints/kube-scheduler uid: 74ec81d5-c588-11e9-b643-000c29327412 在所有master节点上查看功能是否正常1234567kubectl get componentstatusesNAME STATUS MESSAGE ERRORcontroller-manager Healthy ok scheduler Healthy ok etcd-2 Healthy &#123;\"health\":\"true\"&#125; etcd-1 Healthy &#123;\"health\":\"true\"&#125; etcd-0 Healthy &#123;\"health\":\"true\"&#125; 部署nodenode节点运行 docker flannel kubelet kube-proxy 先配置Centos环境,完成安装前配置和Docker，flannel的安装 安装flanneld将master上的文件cp到worker节点，并且安装启动flanneld123ssh -n 192.168.1.35 \"mkdir -p /etc/kubernetes/ssl &amp;&amp; exit\"scp ca.pem 192.168.1.35:/etc/kubernetes/sslscp flanneld*.pem 192.168.1.35:/etc/kubernetes/ssl 之后完成部署flannel中的安装flannel 部署kubeletkubelet运行在每个 worker 节点上，接收 kube-apiserver 发送的请求，管理 Pod 容器，执行交互式命令，如 exec、run、logs 等。kubelet 启动时自动向 kube-apiserver注册节点信息，内置的 cadvisor 统计和监控节点的资源使用情况。 下载解压包，拷贝命令(worker节点) 123wget https://storage.googleapis.com/kubernetes-release/release/v1.14.3/kubernetes-node-linux-amd64.tar.gztar -zxvf kubernetes-node-linux-amd64.tar.gzcp kubectl kubelet kube-proxy /usr/local/bin 创建kubelet-bootstrap.kubeconfig文件,要创建3次分别是(k8s-master1,k8s-master2,k8s-master3),都在master1上执行 k8s-master1: 123456789101112131415161718192021222324252627#创建tokencd /root/sslexport BOOTSTRAP_TOKEN=$(kubeadm token create \\ --description kubelet-bootstrap-token \\ --groups system:bootstrappers:k8s-master1 \\ --kubeconfig ~/.kube/config)#设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.10:8443 \\ --kubeconfig=kubelet-bootstrap-k8s-master1.kubeconfig#设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=kubelet-bootstrap-k8s-master1.kubeconfig#设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubelet-bootstrap-k8s-master1.kubeconfig#设置默认上下文kubectl config use-context default --kubeconfig=kubelet-bootstrap-k8s-master1.kubeconfig k8s-master2: 1234567891011121314151617181920212223242526#创建tokencd /root/sslexport BOOTSTRAP_TOKEN=$(kubeadm token create \\ --description kubelet-bootstrap-token \\ --groups system:bootstrappers:k8s-master2 \\ --kubeconfig ~/.kube/config)#设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.10:8443 \\ --kubeconfig=kubelet-bootstrap-k8s-master2.kubeconfig#设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=kubelet-bootstrap-k8s-master2.kubeconfig#设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubelet-bootstrap-k8s-master2.kubeconfig#设置默认上下文kubectl config use-context default --kubeconfig=kubelet-bootstrap-k8s-master2.kubeconfig k8s-master3: 1234567891011121314151617181920212223242526#创建tokencd /root/sslexport BOOTSTRAP_TOKEN=$(kubeadm token create \\ --description kubelet-bootstrap-token \\ --groups system:bootstrappers:k8s-master3 \\ --kubeconfig ~/.kube/config)#设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.10:8443 \\ --kubeconfig=kubelet-bootstrap-k8s-master3.kubeconfig#设置客户端认证参数kubectl config set-credentials kubelet-bootstrap \\ --token=$&#123;BOOTSTRAP_TOKEN&#125; \\ --kubeconfig=kubelet-bootstrap-k8s-master3.kubeconfig#设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kubelet-bootstrap \\ --kubeconfig=kubelet-bootstrap-k8s-master3.kubeconfig#设置默认上下文kubectl config use-context default --kubeconfig=kubelet-bootstrap-k8s-master3.kubeconfig 查看kubeadm为各节点创建的token(各master节点都可以查看) 1234567891011kubeadm token list --kubeconfig ~/.kube/config#显示如下TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS3exm03.8530h7t1j1v1sfl5 22h 2019-08-28T18:00:05+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:k8s-master16p1ewd.om5m45f26imnd7an 22h 2019-08-28T18:00:05+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:k8s-master2l9yyz0.6g13y8ffsdab9lo5 22h 2019-08-28T18:00:05+08:00 authentication,signing kubelet-bootstrap-token system:bootstrappers:k8s-master3# 如果需要删除创建的tokenkubeadm token --kubeconfig ~/.kube/config delete l9yyz0.6g13y8ffsdab9lo5# 查看各token关联的secretkubectl get secrets -n kube-system 拷贝bootstrap kubeconfig文件到各个node机器上 1234ssh -n 192.168.1.35 \"mkdir -p /etc/kubernetes/ssl &amp;&amp; exit\"scp kubelet-bootstrap-k8s-master1.kubeconfig 192.168.1.35:/etc/kubernetes/ssl/kubelet-bootstrap.kubeconfigscp kubelet-bootstrap-k8s-master2.kubeconfig 192.168.1.35:/etc/kubernetes/ssl/kubelet-bootstrap.kubeconfigscp kubelet-bootstrap-k8s-master3.kubeconfig 192.168.1.35:/etc/kubernetes/ssl/kubelet-bootstrap.kubeconfig 创建kubelet配置文件 1234567891011121314151617181920212223242526272829303132333435363738cd /root/sslcat &gt; kubelet.config.json &lt;&lt;EOF&#123; \"kind\": \"KubeletConfiguration\", \"apiVersion\": \"kubelet.config.k8s.io/v1beta1\", \"authentication\": &#123; \"x509\": &#123; \"clientCAFile\": \"/etc/kubernetes/ssl/ca.pem\" &#125;, \"webhook\": &#123; \"enabled\": true, \"cacheTTL\": \"2m0s\" &#125;, \"anonymous\": &#123; \"enabled\": false &#125; &#125;, \"authorization\": &#123; \"mode\": \"Webhook\", \"webhook\": &#123; \"cacheAuthorizedTTL\": \"5m0s\", \"cacheUnauthorizedTTL\": \"30s\" &#125; &#125;, \"address\": \"192.168.1.35\", \"port\": 10250, \"readOnlyPort\": 0, \"cgroupDriver\": \"cgroupfs\", \"hairpinMode\": \"promiscuous-bridge\", \"serializeImagePulls\": false, \"featureGates\": &#123; \"RotateKubeletClientCertificate\": true, \"RotateKubeletServerCertificate\": true &#125;, \"clusterDomain\": \"cluster.local\", \"clusterDNS\": [\"10.254.0.2\"]&#125;EOF 拷贝到其他主机,注意，可以修改address为本机IP地址 12cp kubelet.config.json /etc/kubernetes/sslscp kubelet.config.json 192.168.1.35:/etc/kubernetes/ssl 创建kubelet.service文件(worker节点) 123456789101112131415161718192021222324252627282930313233mkdir -p /var/log/kubernetes &amp;&amp; mkdir -p /var/lib/kubeletcat &lt;&lt;EOF &gt; /etc/systemd/system/kubelet.service [Unit]Description=Kubernetes KubeletDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceRequires=docker.service[Service]WorkingDirectory=/var/lib/kubeletExecStart=/usr/local/bin/kubelet \\ --bootstrap-kubeconfig=/etc/kubernetes/ssl/kubelet-bootstrap.kubeconfig \\ --cert-dir=/etc/kubernetes/ssl \\ --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d \\ --cni-bin-dir=/usr/local/bin/ \\ --fail-swap-on=false \\ --kubeconfig=/etc/kubernetes/ssl/kubelet-bootstrap.kubeconfig \\ --config=/etc/kubernetes/ssl/kubelet.config.json \\ --hostname-override=192.168.1.35 \\ --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/google_containers/pause-amd64:3.1 \\ --allow-privileged=true \\ --alsologtostderr=true \\ --logtostderr=false \\ --cgroup-driver=systemd \\ --log-dir=/var/log/kubernetes \\ --v=2Restart=on-failureRestartSec=5[Install]WantedBy=multi-user.targetEOF Bootstrap Token Auth 和授予权限 ,需要先将bootstrap-token文件中的kubelet-bootstrap用户赋予system:node-bootstrapper角色，然后kubelet才有权限创建认证请求 1kubectl create clusterrolebinding kubelet-bootstrap --clusterrole=system:node-bootstrapper --group=system:bootstrappers 启动kubele服务 1systemctl daemon-reload &amp;&amp; systemctl enable kubelet &amp;&amp; systemctl restart kubelet &amp;&amp; systemctl status kubelet 检查服务 12345netstat -lantp|grep kubelet# 通过kubelet 的TLS 证书请求，kubelet 首次启动时向kube-apiserver 发送证书签名请求，必须通过后kubernetes 系统才会将该 Node 加入到集群。查看未授权的CSR 请求kubectl get csrNAME AGE REQUESTOR CONDITIONnode-csr-F0yftUyMpWGyDFRPUoGfF5XgbtPFEfyakLidUu9GY6c 99m system:bootstrap:balnwx Pending approve kubelet csr请求(手动和自动选其一) 1.手动approve csr请求(推荐自动的方式) 12345678910111213141516kubectl certificate approve node-csr-YNCI2r5QgwPTj4JR7X0VswSR0klbgG2rZ6R7rb_NIcs#显示certificatesigningrequest.certificates.k8s.io/node-csr-YNCI2r5QgwPTj4JR7X0VswSR0klbgG2rZ6R7rb_NIcs approved#查看结果kubectl describe csr node-csr-YNCI2r5QgwPTj4JR7X0VswSR0klbgG2rZ6R7rb_NIcsName: node-csr-YNCI2r5QgwPTj4JR7X0VswSR0klbgG2rZ6R7rb_NIcsLabels: &lt;none&gt;Annotations: &lt;none&gt;CreationTimestamp: Tue, 27 Aug 2019 17:23:29 +0800Requesting User: system:bootstrap:balnwxStatus: Approved,IssuedSubject: Common Name: system:node:192.168.1.35 Serial Number: Organization: system:nodesEvents: &lt;none&gt; 2.自动approve csr请求方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#创建ClusterRoleBinding，分别用于自动 approve client、renew client、renew server 证书cd /root/sslcat &gt; csr-crb.yaml &lt;&lt;EOF# Approve all CSRs for the group \"system:bootstrappers\" kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: auto-approve-csrs-for-group subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclient apiGroup: rbac.authorization.k8s.io--- # To let a node of the group \"system:bootstrappers\" renew its own credentials kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: node-client-cert-renewal subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient apiGroup: rbac.authorization.k8s.io---# A ClusterRole which instructs the CSR approver to approve a node requesting a# serving cert matching its client cert.kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1metadata: name: approve-node-server-renewal-csrrules:- apiGroups: [\"certificates.k8s.io\"] resources: [\"certificatesigningrequests/selfnodeserver\"] verbs: [\"create\"]--- # To let a node of the group \"system:nodes\" renew its own server credentials kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: node-server-cert-renewal subjects: - kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: approve-node-server-renewal-csr apiGroup: rbac.authorization.k8s.ioEOF#拷贝到其他master节点上cp csr-crb.yaml /etc/kubernetes/sslscp csr-crb.yaml 192.168.1.32:/etc/kubernetes/sslscp csr-crb.yaml 192.168.1.33:/etc/kubernetes/ssl#生效配置kubectl apply -f /etc/kubernetes/ssl/csr-crb.yaml 查看123kubectl get --all-namespaces -o wide nodesNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME192.168.1.35 Ready &lt;none&gt; 7m v1.14.3 192.168.1.35 &lt;none&gt; CentOS Linux 7 (Core) 3.10.0-957.27.2.el7.x86_64 docker://19.3.1 部署kube-proxykube-proxy 运行在所有 worker 节点上，，它监听 apiserver 中 service 和 Endpoint 的变化情况，创建路由规则来进行服务负载均衡。 创建kube-proxy证书 12345678910111213141516171819cd /root/sslcat &gt; kube-proxy-csr.json &lt;&lt;EOF&#123; \"CN\": \"system:kube-proxy\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"ShangHai\", \"L\": \"ShangHai\", \"O\": \"k8s\", \"OU\": \"System\" &#125; ]&#125;EOF 生成证书和私钥 1234cfssl gencert -ca=ca.pem \\ -ca-key=ca-key.pem \\ -config=ca-config.json \\ -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy 创建kubeconfig文件 12345678910111213141516171819#1.设置集群参数kubectl config set-cluster kubernetes \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.1.10:8443 \\ --kubeconfig=kube-proxy.kubeconfig#2.设置客户端认证参数kubectl config set-credentials kube-proxy \\ --client-certificate=kube-proxy.pem \\ --client-key=kube-proxy-key.pem \\ --embed-certs=true \\ --kubeconfig=kube-proxy.kubeconfig#3.设置上下文参数kubectl config set-context default \\ --cluster=kubernetes \\ --user=kube-proxy \\ --kubeconfig=kube-proxy.kubeconfig#4.设置默认上下文kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig 拷贝到worker节点 1scp kube-proxy*.pem kube-proxy.kubeconfig 192.168.1.35:/etc/kubernetes/ssl/ 创建kube-proxy配置文件 12345678910111213cd /root/sslcat &gt;kube-proxy.config.yaml &lt;&lt;EOFapiVersion: kubeproxy.config.k8s.io/v1alpha1bindAddress: 192.168.1.35clientConnection: kubeconfig: /etc/kubernetes/ssl/kube-proxy.kubeconfigclusterCIDR: 172.30.0.0/16healthzBindAddress: 192.168.1.35:10256hostnameOverride: 192.168.1.35kind: KubeProxyConfigurationmetricsBindAddress: 192.168.1.35:10249mode: \"ipvs\"EOF 拷贝到其他节点 1scp kube-proxy.config.yaml 192.168.1.35:/etc/kubernetes/ssl/ 创建kube-proxy.service文件(worker节点) 123456789101112131415161718192021cat &lt;&lt; EOF &gt; /etc/systemd/system/kube-proxy.service[Unit]Description=Kubernetes Kube-Proxy ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=network.target[Service]WorkingDirectory=/var/lib/kube-proxyExecStart=/usr/local/bin/kube-proxy \\ --config=/etc/kubernetes/ssl/kube-proxy.config.yaml \\ --alsologtostderr=true \\ --logtostderr=false \\ --log-dir=/var/log/kubernetes/kube-proxy \\ --v=2Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF 启动kube-proxy服务(worker节点) 12mkdir -p /var/lib/kube-proxy &amp;&amp; mkdir -p /var/log/kubernetes/kube-proxysystemctl daemon-reload &amp;&amp; systemctl enable kube-proxy &amp;&amp; systemctl restart kube-proxy &amp;&amp; systemctl status kube-proxy 检查 123456789101112131415netstat -lnpt|grep kube-proxytcp 0 0 192.168.1.35:10249 0.0.0.0:* LISTEN 1031/kube-proxy tcp 0 0 192.168.1.35:10256 0.0.0.0:* LISTEN 1031/kube-proxy ipvsadm -ln#显示如下IP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.254.0.1:443 rr -&gt; 192.168.1.31:6443 Masq 1 0 0 -&gt; 192.168.1.32:6443 Masq 1 0 0 -&gt; 192.168.1.33:6443 Masq 1 0 0 TCP 10.254.189.67:80 rr -&gt; 172.30.87.3:80 Masq 1 0 0 测试集群可用性 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#创建一个podkubectl run nginx --image=nginx#查看pod状态kubectl get pod -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-7db9fccd9b-glrx5 1/1 Running 0 27m 172.30.87.3 192.168.1.35 &lt;none&gt; &lt;none&gt;#测试IP是否ping通ping -c4 172.30.87.3PING 172.30.87.3 (172.30.87.3) 56(84) bytes of data.64 bytes from 172.30.87.3: icmp_seq=1 ttl=63 time=0.372 ms64 bytes from 172.30.87.3: icmp_seq=2 ttl=63 time=0.188 ms64 bytes from 172.30.87.3: icmp_seq=3 ttl=63 time=0.160 ms64 bytes from 172.30.87.3: icmp_seq=4 ttl=63 time=0.169 ms--- 172.30.87.3 ping statistics ---4 packets transmitted, 4 received, 0% packet loss, time 2999msrtt min/avg/max/mdev = 0.160/0.222/0.372/0.087 ms#创建服务kubectl expose deployment nginx --name=nginx --port=80 --target-port=80 --type=NodePortservice/nginx exposed#查看服务kubectl get svc -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkubernetes ClusterIP 10.254.0.1 &lt;none&gt; 443/TCP 2d1h &lt;none&gt;nginx NodePort 10.254.215.97 &lt;none&gt; 80:31401/TCP 31s run=nginx#访问curl访问node_ip：nodeportcurl -I 192.168.1.35:31401HTTP/1.1 200 OKServer: nginx/1.17.3Date: Wed, 28 Aug 2019 10:06:40 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 13 Aug 2019 08:50:00 GMTConnection: keep-aliveETag: \"5d5279b8-264\"Accept-Ranges: bytes#在flannel worker主机上访问集群IPip add | grep 10.254 inet 10.254.0.1/32 brd 10.254.0.1 scope global kube-ipvs0 inet 10.254.189.67/32 brd 10.254.189.67 scope global kube-ipvs0 inet 10.254.215.97/32 brd 10.254.215.97 scope global kube-ipvs0curl -I http://10.254.189.67:80HTTP/1.1 200 OKServer: nginx/1.17.3Date: Wed, 28 Aug 2019 10:10:26 GMTContent-Type: text/htmlContent-Length: 612Last-Modified: Tue, 13 Aug 2019 08:50:00 GMTConnection: keep-aliveETag: \"5d5279b8-264\"Accept-Ranges: bytes 部署coredns插件插件是集群的附件组件，丰富和完善了集群的功能 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#将kubernetes-server-linux-amd64.tar.gz解压后，再解压其中的 kubernetes-src.tar.gz 文件,获取coredns配置文件tar -zxvf kubernetes-server-linux-amd64.tar.gzcd kubernetesmkdir srctar -zxvf kubernetes-src.tar.gz -C srccd src/cluster/addons/dns/corednscp coredns.yaml.base /etc/kubernetes/coredns.yamlsed -i \"s/__PILLAR__DNS__DOMAIN__/cluster.local/g\" /etc/kubernetes/coredns.yamlsed -i \"s/__PILLAR__DNS__SERVER__/10.254.0.2/g\" /etc/kubernetes/coredns.yaml#创建corednskubectl create -f /etc/kubernetes/coredns.yamlserviceaccount/coredns createdclusterrole.rbac.authorization.k8s.io/system:coredns createdclusterrolebinding.rbac.authorization.k8s.io/system:coredns createdconfigmap/coredns createddeployment.apps/coredns createdservice/kube-dns created#检查codedns功能kubectl -n kube-system get all -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESpod/coredns-5b969f4c88-7l7c9 0/1 ImagePullBackOff 0 4m18s 172.30.87.4 192.168.1.35 &lt;none&gt; &lt;none&gt;NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice/kube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 4m18s k8s-app=kube-dnsNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORdeployment.apps/coredns 0/1 1 0 4m18s coredns k8s.gcr.io/coredns:1.3.1 k8s-app=kube-dnsNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORreplicaset.apps/coredns-5b969f4c88 1 1 0 4m18s coredns k8s.gcr.io/coredns:1.3.1 k8s-app=kube-dns,pod-template-hash=5b969f4c88# ImagePullBackOff 镜像下载失败，修改sed -i \"s/k8s.gcr.io/coredns/g\" /etc/kubernetes/coredns.yamlkubectl apply -f /etc/kubernetes/coredns.yaml Warning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applyserviceaccount/coredns configuredWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applyclusterrole.rbac.authorization.k8s.io/system:coredns configuredWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applyclusterrolebinding.rbac.authorization.k8s.io/system:coredns configuredWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applyconfigmap/coredns configuredWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applydeployment.apps/coredns configuredWarning: kubectl apply should be used on resource created by either kubectl create --save-config or kubectl applyservice/kube-dns configured# 再次查看kubectl -n kube-system get all -o wide 部署dashboard插件将kubernetes-server-linux-amd64.tar.gz 解压后，再解压其中的 kubernetes-src.tar.gz 文件。dashboard 对应的目录是：cluster/addons/dashboard ，拷贝dashboard的文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445#配置文件cd kubernetes/src/cluster/addons/dashboardmkdir -p /etc/kubernetes/dashboardcp *.yaml /etc/kubernetes/dashboard/cd /etc/kubernetes/dashboardsed -i \"s@image:.*@image: registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64:v1.10.1@g\" dashboard-controller.yamlsed -i \"/spec/a\\ type: NodePort\" dashboard-service.yamlsed -i \"/targetPort/a\\ nodePort: 32700\" dashboard-service.yaml#执行所有kubectl create -f /etc/kubernetes/dashboardconfigmap/kubernetes-dashboard-settings createdserviceaccount/kubernetes-dashboard createddeployment.apps/kubernetes-dashboard createdrole.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdrolebinding.rbac.authorization.k8s.io/kubernetes-dashboard-minimal createdsecret/kubernetes-dashboard-certs createdsecret/kubernetes-dashboard-key-holder createdservice/kubernetes-dashboard created#查看分配的NodePortkubectl -n kube-system get all -o widekubectl get pod -o wide -n kube-system NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATEScoredns-8854569d4-7w7gb 1/1 Running 0 21m 172.30.87.5 192.168.1.35 &lt;none&gt; &lt;none&gt;kubernetes-dashboard-7d5f7c58f5-c5nxc 1/1 Running 0 3m6s 172.30.87.7 192.168.1.35 &lt;none&gt; &lt;none&gt;kubectl get svc -o wide -n kube-system NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORkube-dns ClusterIP 10.254.0.2 &lt;none&gt; 53/UDP,53/TCP,9153/TCP 28m k8s-app=kube-dnskubernetes-dashboard NodePort 10.254.219.188 &lt;none&gt; 443:32700/TCP 3m13s k8s-app=kubernetes-dashboard#此时可访问dashboard https://192.168.1.35:32700，但需要口令，使用帮助命令kubectl exec -n kube-system -it kubernetes-dashboard-7d5f7c58f5-c5nxc -- /dashboard --help#创建登录tokenkubectl create sa dashboard-admin -n kube-systemserviceaccount/dashboard-admin createdkubectl create clusterrolebinding dashboard-admin --clusterrole=cluster-admin --serviceaccount=kube-system:dashboard-adminclusterrolebinding.rbac.authorization.k8s.io/dashboard-admin createdADMIN_SECRET=$(kubectl get secrets -n kube-system | grep dashboard-admin | awk '&#123;print $1&#125;')DASHBOARD_LOGIN_TOKEN=$(kubectl describe secret -n kube-system $&#123;ADMIN_SECRET&#125; | grep -E '^token' | awk '&#123;print $2&#125;')echo $&#123;DASHBOARD_LOGIN_TOKEN&#125; # 使用输出的DASHBOARD_LOGIN_TOKEN登录 本文来源参照 本文来源参照 本文来源参照","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/tags/kubernetes/"}]},{"title":"hive添加用户权限","slug":"hive添加用户权限","date":"2019-07-01T16:00:00.000Z","updated":"2019-10-14T07:39:31.008Z","comments":true,"path":"2019/07/02/hive添加用户权限/","link":"","permalink":"https://ngames-dev.cn/2019/07/02/hive添加用户权限/","excerpt":"在前面hadoop+hive+hbase环境里面，hive部分简单的配置了基于MySQL的本地模式安装但是考虑到安全，需要给hive添加认证登陆而且，使用hive命令beeline链接hive，也是强行需要密码的 在之前的hive-site.xml配置中，hive.server2.authentication 的配置为NONE，表示没有用户认证。而HiveServer2支持多种用户安全认证方式：NONE,NOSASL, KERBEROS, LDAP, PAM ,CUSTOM等等，在此，我们使用CUSTOM自定义安全认证","text":"在前面hadoop+hive+hbase环境里面，hive部分简单的配置了基于MySQL的本地模式安装但是考虑到安全，需要给hive添加认证登陆而且，使用hive命令beeline链接hive，也是强行需要密码的 在之前的hive-site.xml配置中，hive.server2.authentication 的配置为NONE，表示没有用户认证。而HiveServer2支持多种用户安全认证方式：NONE,NOSASL, KERBEROS, LDAP, PAM ,CUSTOM等等，在此，我们使用CUSTOM自定义安全认证 配置安全认证代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package org.apache.hive;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.hive.conf.HiveConf;import org.apache.hive.service.auth.PasswdAuthenticationProvider;import org.slf4j.Logger;import javax.security.sasl.AuthenticationException;/** * @author SakuraGaara * @date 2019/07/01 */public class CustomPasswdAuthenticator implements PasswdAuthenticationProvider &#123; private Logger LOG = org.slf4j.LoggerFactory.getLogger(CustomPasswdAuthenticator.class); private static final String HIVE_JDBC_PASSWD_AUTH_PREFIX = \"hive.jdbc_passwd.auth.%s\"; private Configuration conf = null; @Override public void Authenticate(String userName, String passwd) throws AuthenticationException &#123; LOG.info(\"user: \" + userName + \" try login.\"); String passwdConf = getConf().get(String.format(HIVE_JDBC_PASSWD_AUTH_PREFIX, userName)); if (passwdConf == null) &#123; String message = \"user's ACL configration is not found. user:\" + userName; LOG.info(message); throw new AuthenticationException(message); &#125; if (!passwd.equals(passwdConf)) &#123; String message = \"user name and password is mismatch. user:\" + userName; throw new AuthenticationException(message); &#125; &#125; public Configuration getConf() &#123; if (conf == null) &#123; this.conf = new Configuration(new HiveConf()); &#125; return conf; &#125; public void setConf(Configuration conf) &#123; this.conf = conf; &#125;&#125; 将以上代码打包hive-1.0-SNAPSHOT.jar，将jar放置$HIVE_HOME/lib目录 修改hive-site.xml文件配置hive.server2.authentication // 指定认证方式CUSTOMhive.server2.custom.authentication.class // 指定认证方式接口hive.jdbc_passwd.auth // 设置用户为admin，value为用户密码 123456789101112131415161718&lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;CUSTOM&lt;/value&gt; &lt;description&gt; Expects one of [nosasl, none, ldap, kerberos, pam, custom]. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.custom.authentication.class&lt;/name&gt; &lt;value&gt;org.apache.hive.CustomPasswdAuthenticator&lt;/value&gt; &lt;description&gt; Custom authentication class. Used when property &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.jdbc_passwd.auth.admin&lt;/name&gt; &lt;value&gt;12345678&lt;/value&gt;&lt;/property&gt; 重新启动hiveserver2,此时可jps看出RunJar进程，则启动成功 1nohup ./bin/hiveserver2 &gt; /dev/null 2&gt;&amp;1 &amp; 验证使用beeline命令链接，此时需要用户密码 123456789101112shell&gt; beeline SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/yibao/data/app/hive-2.3.5/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/yibao/data/app/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Beeline version 2.3.5 by Apache Hivebeeline&gt; !connect jdbc:hive2://master:10000Connecting to jdbc:hive2://master:10000Enter username for jdbc:hive2://master:10000: adminEnter password for jdbc:hive2://master:10000: ********Error: Could not open client transport with JDBC Uri: jdbc:hive2://master:10000: Failed to open new session: java.lang.RuntimeException: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.authorize.AuthorizationException): User: admin is not allowed to impersonate (state=08S01,code=0) 此时会发现，虽然配置了hive认证，但是使用beeline或者其他客户端，依旧无法连接hive 解决方案1，首先确定hive是否开启2，然后再hadoop下的core-site.xml加入配置 12345678&lt;property&gt; &lt;name&gt;hadoop.proxyuser.admin.hosts&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hadoop.proxyuser.admin.groups&lt;/name&gt; &lt;value&gt;*&lt;/value&gt;&lt;/property&gt; 注意配置中的“admin”代表的是一个用户，你异常中User后面的用户写的是哪个，你在这里就配置哪个。hadoop.proxyuser.admin.hosts 配置成*的意义，表示任意节点使用hadoop集群的代理用户admin都能访问hdfs集群hadoop.proxyuser.admin.groups 表示代理用户的组所属 3，然后在hadoop下的hdfs-site.xml中加入配置 1234&lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt;&lt;/property&gt; 4, 重启hadoop集群和hive,然后beeline重新连接hive 1234567891011121314151617181920212223shell&gt; beeline SLF4J: Class path contains multiple SLF4J bindings.SLF4J: Found binding in [jar:file:/yibao/data/app/hive-2.3.5/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: Found binding in [jar:file:/yibao/data/app/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]Beeline version 2.3.5 by Apache Hivebeeline&gt; !connect jdbc:hive2://master:10000 Connecting to jdbc:hive2://master:10000Enter username for jdbc:hive2://master:10000: adminEnter password for jdbc:hive2://master:10000: ********Connected to: Apache Hive (version 2.3.5)Driver: Hive JDBC (version 2.3.5)Transaction isolation: TRANSACTION_REPEATABLE_READ0: jdbc:hive2://master:10000&gt; show databases;+----------------+| database_name |+----------------+| default || sakura |+----------------+2 rows selected (0.341 seconds)0: jdbc:hive2://master:10000&gt;","categories":[{"name":"大数据","slug":"大数据","permalink":"https://ngames-dev.cn/categories/大数据/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://ngames-dev.cn/tags/hive/"}]},{"title":"hadoop+hive+hbase环境","slug":"hadoop+hive+hbase环境","date":"2019-06-29T16:00:00.000Z","updated":"2019-09-06T09:23:30.125Z","comments":true,"path":"2019/06/30/hadoop+hive+hbase环境/","link":"","permalink":"https://ngames-dev.cn/2019/06/30/hadoop+hive+hbase环境/","excerpt":"相关软件准备 apache-hive-2.3.5-bin.tar.gz hadoop-2.7.7.tar.gz hbase-1.3.5-bin.tar.gz mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz mysql-connector-java-5.1.42-bin.jar zookeeper-3.4.14.tar.gz 环境介绍 准备三台服务器，修改hostname主机名，分别为master, slave1, slave2,并将三台主机名添加到/etc/hosts中 12310.27.214.15 slave110.26.234.215 slave210.26.108.150 master 准备好相关的安装目录，mkdir /yibao/data/app (所有的项目都会安装在次目录下) 配置三台服务器之间无密码登陆","text":"相关软件准备 apache-hive-2.3.5-bin.tar.gz hadoop-2.7.7.tar.gz hbase-1.3.5-bin.tar.gz mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz mysql-connector-java-5.1.42-bin.jar zookeeper-3.4.14.tar.gz 环境介绍 准备三台服务器，修改hostname主机名，分别为master, slave1, slave2,并将三台主机名添加到/etc/hosts中 12310.27.214.15 slave110.26.234.215 slave210.26.108.150 master 准备好相关的安装目录，mkdir /yibao/data/app (所有的项目都会安装在次目录下) 配置三台服务器之间无密码登陆 配置Java环境三台都需要配置 123456JAVA_HOME=/usr/local/java/jdk1.8.0_60/JAVA_BIN=/usr/local/java/jdk1.8.0_60/binJRE_HOME=/usr/local/java/jdk1.8.0_60/jrePATH=$PATH:/usr/local/java/jdk1.8.0_60/bin:/usr/local/java/jdk1.8.0_60/jre/binCLASSPATH=/usr/local/java/jdk1.8.0_60/jre/lib:/usr/local/java/jdk1.8.0_60/lib:/usr/local/java/jdk1.8.0_60/jre/lib/charsets.jarexport JAVA_HOME JAVA_BIN JRE_HOME PATH CLASSPATH 安装hadoop集群解压hadoop到相关安装目录1tar zxvf hadoop-2.7.7.tar.gz -C /yibao/data/app/ 添加hadoop环境变量/etc/profile 1export HADOOP_HOME=/yibao/data/app/hadoop-2.7.7 配置hadoop中的配置文件主要配置四个配置文件，在hadoop-2.7.7/etc/hadoop目录中，分别为core-site.xmlhdfs-site.xmlyarn-site.xmlmapred-site.xml(由mapred-site.xml.template拷贝）slaveshadoop-env.sh 修改core-site.xml 12345678910&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/yibao/data/app/hadoop-2.7.7/tmp&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://master:9000&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改hdfs-site.xml 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt; &lt;value&gt;master:9001&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:/yibao/data/app/hadoop-2.7.7/namenode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:/yibao/data/app/hadoop-2.7.7/datanode&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.webhdfs.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.permissions&lt;/name&gt; &lt;value&gt;false&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.web.ugi&lt;/name&gt; &lt;value&gt;supergroup&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml 1234567891011121314&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt; &lt;value&gt;master:10020&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt; &lt;value&gt;master:19888&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt; &lt;value&gt;mapreduce_shuffle&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt; &lt;value&gt;master:8032&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt; &lt;value&gt;master:8030&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt; &lt;value&gt;master:8031&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt; &lt;value&gt;master:8033&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt; &lt;value&gt;master:8078&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; slaves 12slave1slave2 hadoop-env.sh 12export JAVA_HOME=/usr/local/java/jdk1.8.0_60export HADOOP_SSH_OPTS=\"-p 22222\" #由于三台主机的ssh端口都是22222,所以此处添加 将目录cp到slave1,slave2两台服务器中 12scp -r -P 22222 hadoop-2.7.7 slave1:/yibao/data/app/scp -r -P 22222 hadoop-2.7.7 slave2:/yibao/data/app/ 在slave1,slave2中/etc/profile添加HADOOP_HOME环境变量 12export HADOOP_HOME=/yibao/data/app/hadoop-2.7.7source /etc/profile 验证并启动hadoop 验证，在master节点中初始化namenode节点,确认无误后，启动hadoop集群 12./bin/hadoop namenode -format #初始化namenode节点./sbin/start-all.sh #启动hadoop集群 之后可以使用jps命令查看每台机器上的Java进程master节点： 123428368 Jps11287 SecondaryNameNode11534 ResourceManager11055 NameNode slave1节点: 12332020 DataNode13626 Jps32142 NodeManager slave2节点： 1232608 NodeManager2469 DataNode21542 Jps 此时可查看NameNode进程端口50070,访问 http://master:50070,可以看到熟悉的Hadoop界面 安装mysql解压mysql并安装12345678910sudo -i #使用root用户安装tar zxvf mysql-5.7.22-linux-glibc2.12-x86_64.tar.gz -C /yibao/data/app/cd /yibao/data/appmv mysql-5.7.22-linux-glibc2.12-x86_64 mysqlcd mysqlgroupadd mysqlgroupadd mysqluseradd -r -g mysql -s /sbin/nologin mysqlchown -R mysql.mysql ../bin/mysqld --initialize --user=mysql --basedir=/yibao/data/app/mysql --datadir=/yibao/data/app/mysql/data #返回root@localhost密码 创建mysql配置文件vim /etc/my.cnf 12345678910111213[mysqld]basedir=/yibao/data/app/mysql datadir=/yibao/data/app/mysql/dataport=3306character_set_server=utf8socket=/tmp/mysql.sock#skip-grant-tables#innodb_buffer_pool_size=1Ginnodb_log_file_size=256Mmax_allowed_packet=64Msql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES mysql加入服务12345cp support-files/mysql.server /etc/init.d/mysqldchmod +x /etc/init.d/mysqld chkconfig --add mysqldchkconfig mysqld onservice mysqld start 安装hive基于MySQL的本地模式安装（hive只需安装master即可） 解压hive安装包123tar zxvf apache-hive-2.3.5-bin.tar.gz -C /yibao/data/app/cd /yibao/data/appmv apache-hive-2.3.5-bin hive-2.3.5 配置hive 将mysql驱动包放置hive-2.3.5/lib目录 1cp mysql-connector-java-5.1.42-bin.jar hive-2.3.5/lib/ 登陆mysql创建hive链接需要的账号，和数据库 1234mysql -uroot -p -hlocalhostmysql&gt; GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost' IDENTIFIED BY \"hive_password\";mysql&gt; FLUSH PRIVILEGES;mysql&gt; CREATE DATABASE hive; 配置hive环境变量将以下加入/etc/profile文件 12export HIVE_HOME=/yibao/data/app/hive-2.3.5PATH=$PATH:$HIVE_HOME/bin 配置hive配置文件hive-site.xml(由hive-default.xml.template复制生成) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455&lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/yibao/data/app/hivedata&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.authentication&lt;/name&gt; &lt;value&gt;NONE&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt; &lt;description&gt;Driver class name for a JDBC metastore&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;useSSL=false&lt;/value&gt; &lt;description&gt; JDBC connect string for a JDBC metastore. To use SSL to encrypt/authenticate the connection, provide database-specific SSL flag in the connection URL. For example, jdbc:postgresql://myhost/db?ssl=true for postgres database. &lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hive&lt;/value&gt; &lt;description&gt;Username to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hive_password&lt;/value&gt; &lt;description&gt;password to use against metastore database&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.exec.local.scratchdir&lt;/name&gt; &lt;value&gt;/yibao/data/app/hive_tmp/HiveJobsLog&lt;/value&gt; &lt;description&gt;Local scratch space for Hive jobs&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.downloaded.resources.dir&lt;/name&gt; &lt;value&gt;/yibao/data/app/hive_tmp/resources&lt;/value&gt; &lt;description&gt;Temporary local directory for added resources in the remote file system.&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/yibao/data/app/hive_tmp/HiveRunLog&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.server2.logging.operation.log.location&lt;/name&gt; &lt;value&gt;/yibao/data/app/hive_tmp/OpertitionLog&lt;/value&gt; &lt;description&gt;Top level directory where operation logs are stored if logging functionality is enabled&lt;/description&gt;&lt;/property&gt; 创建hivedata,hive_tmp文件夹 1mkdir -p /yibao/data/app/hivedata /yibao/data/app/hive_tmp 修改hive-env.sh文件（由hive-env.sh.template拷贝生成） 12345export JAVA_HOME=/usr/local/java/jdk1.8.0_60export HADOOP_HOME=/yibao/data/app/hadoop-2.7.7export HIVE_HOME=/yibao/data/app/hive-2.3.5export HIVE_CONF_DIR=$HIVE_HOME/confexport HIVE_AUX_JARS_PATH=$HIVE_HOME/lib 初始化并启动 hive初始化元数据 1bin/schematool -initSchema -dbType mysql 使用hive命令启动hive 1234hive&gt; show databases;OKdefaultTime taken: 7.743 seconds, Fetched: 1 row(s) 安装zookeeper集群zookeeper集群三台都需要安装,先配置master,之后scp到slave 解压zookeeper123tar zxvf zookeeper-3.4.14.tar.gz -C /yibao/data/app/cd /yibao/data/app/zookeeper-3.4.14/confcp zoo_sample.cfg zoo.cfg 配置zookeeper 修改zoo.cfg 12345678tickTime=2000initLimit=10syncLimit=5dataDir=/yibao/data/app/zookeeper-3.4.14/dataclientPort=2181server.0=master:2888:3888server.1=slave1:2888:3888server.2=slave2:2888:3888 创建zookeeper data目录 12mkdir /yibao/data/app/zookeeper-3.4.14/dataecho 0 &gt; /yibao/data/app/zookeeper-3.4.14/data/myid 将zookeeper-3.4.14同步至slave 12scp -r -P 22222 zookeeper-3.4.14 slave1:/yibao/data/app/scp -r -P 22222 zookeeper-3.4.14 slave2:/yibao/data/app/ 修改zookeeper idslave1 下/yibao/data/app/zookeeper-3.4.14/data/myid 改为1slave2 下/yibao/data/app/zookeeper-3.4.14/data/myid 改为2 配置zookeeper环境变量,加入/etc/profile 12export ZOOKEEPER_HOME=/yibao/data/app/zookeeper-3.4.14export PATH=$PATH:$ZOOKEEPER_HOME/bin 启动zookeepermaster: ./bin/zkServer.sh startslave1: ./bin/zkServer.sh startslave2: ./bin/zkServer.sh start 验证master上执行 12345678910./bin/zkCli.sh2019-06-30 15:30:12,558 [myid:] - INFO [main:Environment@100] - Client .......................WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0] create /sakura GaaraCreated /sakura[zk: localhost:2181(CONNECTED) 1] get /sakuraGaara...................[zk: localhost:2181(CONNECTED) 2] slave1, slave2上去验证 123456789./bin/zkCli.shConnecting to localhost:21812019-06-30 15:33:03,582 [myid:] - INFO [main:Environment@100] - Client .......................WatchedEvent state:SyncConnected type:None path:null[zk: localhost:2181(CONNECTED) 0] get /sakuraGaara...........[zk: localhost:2181(CONNECTED) 1] 安装hbase集群解压hbase12tar zxvf hbase-1.3.5-bin.tar.gz -C /yibao/data/appcd hbase-1.3.5/conf 配置hbase 修改hbase-site.xml 123456789101112131415161718192021222324252627282930&lt;configuration&gt; &lt;property&gt; &lt;name&gt;hbase.rootdir&lt;/name&gt; &lt;value&gt;hdfs://master:9000/yibao/data/app/hbase-1.3.5&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.cluster.distributed&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.master&lt;/name&gt; &lt;value&gt;master:60000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.dataDir&lt;/name&gt; &lt;value&gt;/yibao/data/app/hbase-1.3.5/zookeeperdata&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.quorum&lt;/name&gt; &lt;value&gt;master,slave1,slave2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.zookeeper.property.clientPort&lt;/name&gt; &lt;value&gt;2181&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.tmp.dir&lt;/name&gt; &lt;value&gt;/yibao/data/app/hbase-1.3.5/tmpdata&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 在conf下创建backup-masters文件，设置slave1为Backup Masters 1echo \"slave1\" &gt; backup-masters 修改regionservers，设置slave1,slave2为Region Servers 123cat regionservers slave1slave2 修改hbase-env.sh 1234567export HBASE_SSH_OPTS=\"-p 22222\" # 由于ssh端口是22222，所以此处添加export JAVA_HOME=/usr/local/java/jdk1.8.0_60export HBASE_CLASSPATH=/yibao/data/app/hbase-1.3.5/confexport HBASE_MANAGES_ZK=falseexport HBASE_HOME=/yibao/data/app/hbase-1.3.5export HADOOP_HOME=/yibao/data/app/hadoop-2.7.7export HBASE_LOG_DIR=/yibao/data/app/hbase-1.3.5/logs 创建hbase配置所需的文件夹 12cd /yibao/data/app/hbase-1.3.5mkdir -p tmpdata zookeeperdata 将hbase-1.3.5同步到slave1,slave2 12scp -r -P 22222 hbase-1.3.5 slave1:/yibao/data/app/scp -r -P 22222 hbase-1.3.5 slave2:/yibao/data/app/ 三台服务器配置hbase环境变量 12export HBASE_HOME=/yibao/data/app/hbase-1.3.5export PATH=$PATH:$HBASE_HOME/bin 启动hbase master上启动hbase,启动之前确认ZK已经启动 12cd $HBASE_HOMEbin/start-hbase.sh 启动后jps验证master: jps启动的HMaster进程为hbase进程 12345625588 HMaster4678 Jps11287 SecondaryNameNode6552 QuorumPeerMain11534 ResourceManager11055 NameNode slave1: jps启动的HMaster进程为配置slave1为Backup Masters，HRegionServer进程为Region Servers 123456724402 Jps10402 HRegionServer11267 QuorumPeerMain32020 DataNode31546 Bootstrap10508 HMaster32142 NodeManager slave2: jps启动的HRegionServer进程为Region Servers 1234562608 NodeManager2388 Bootstrap2469 DataNode14135 QuorumPeerMain18442 HRegionServer32431 Jps web界面访问master主机HMaster进程的端口16010也可以看出详细的信息，显示Master，Region Servers，Backup Masters，Tables等更多的详细信息","categories":[{"name":"大数据","slug":"大数据","permalink":"https://ngames-dev.cn/categories/大数据/"}],"tags":[{"name":"hive","slug":"hive","permalink":"https://ngames-dev.cn/tags/hive/"},{"name":"hadoop","slug":"hadoop","permalink":"https://ngames-dev.cn/tags/hadoop/"},{"name":"hbase","slug":"hbase","permalink":"https://ngames-dev.cn/tags/hbase/"}]},{"title":"nginx配置url重写","slug":"nginx配置url重写","date":"2019-06-05T16:00:00.000Z","updated":"2019-09-06T09:23:30.125Z","comments":true,"path":"2019/06/06/nginx配置url重写/","link":"","permalink":"https://ngames-dev.cn/2019/06/06/nginx配置url重写/","excerpt":"url重写是指通过配置conf文件，以让网站的url中达到某种状态时则定向/跳转到某个规则比如常见的伪静态、301重定向、浏览器定向等 rewrite语法在配置文件的server块中写，如： 123server &#123; rewrite 规则 定向路径 重写类型;&#125;","text":"url重写是指通过配置conf文件，以让网站的url中达到某种状态时则定向/跳转到某个规则比如常见的伪静态、301重定向、浏览器定向等 rewrite语法在配置文件的server块中写，如： 123server &#123; rewrite 规则 定向路径 重写类型;&#125; 规则：可以是字符串或者正则来表示想匹配的目标url定向路径：表示匹配到规则后要定向的路径，如果规则里有正则，则可以使用$index来表示正则里的捕获分组重写类型：last 相当于Apache里德(L)标记，表示完成rewrite，浏览器地址栏URL地址不变break 本条规则匹配完成后，终止匹配，不再匹配后面的规则，浏览器地址栏URL地址不变redirect 返回302临时重定向，浏览器地址会显示跳转后的URL地址permanent 返回301永久重定向，浏览器地址栏会显示跳转后的URL地址 简单例子 12345678910111213141516171819server &#123; # 访问 /last.html 的时候，页面内容重写到 /index.html 中 rewrite /last.html /index.html last; # 访问 /break.html 的时候，页面内容重写到 /index.html 中，并停止后续的匹配 rewrite /break.html /index.html break; # 访问 /redirect.html 的时候，页面直接302定向到 /index.html中 rewrite /redirect.html /index.html redirect; # 访问 /permanent.html 的时候，页面直接301定向到 /index.html中 rewrite /permanent.html /index.html permanent; # 把 /html/*.html =&gt; /post/*.html ，301定向 rewrite ^/html/(.+?).html$ /post/$1.html permanent; # 把 /search/key =&gt; /search.html?keyword=key rewrite ^/search\\/([^\\/]+?)(\\/|$) /search.html?keyword=$1 permanent;&#125; last和break的区别因为301和302不能简单的只返回状态码，还必须有重定向的URL，这就是return指令无法返回301,302的原因了。这里 last 和 break 区别有点难以理解 last一般写在server和if中，而break一般使用在location中last不终止重写后的url匹配，即新的url会再从server走一遍匹配流程，而break终止重写后的匹配break和last都能组织继续执行后面的rewrite指令在location里一旦返回break则直接生效并停止后续的匹配location 12345678910server &#123; location / &#123; rewrite /last/ /q.html last; rewrite /break/ /q.html break; &#125; location = /q.html &#123; return 400; &#125;&#125; 访问/last/时重写到/q.html，然后使用新的uri再匹配，正好匹配到locatoin = /q.html然后返回了400访问/break时重写到/q.html，由于返回了break，则直接停止了 if判断只是上面的简单重写很多时候满足不了需求，比如需要判断当文件不存在时、当路径包含xx时等条件，则需要用到if 语法 12if (表达式) &#123;&#125; 当表达式只是一个变量时，如果值为空或任何以0开头的字符串都会当做false直接比较变量和内容时，使用=或!=~正则表达式匹配，~*不区分大小写的匹配，!~区分大小写的不匹配一些内置的条件判断：-f和!-f用来判断是否存在文件-d和!-d用来判断是否存在目录-e和!-e用来判断是否存在文件或目录-x和!-x用来判断文件是否可执行 内置的全局变量 123456789101112131415161718192021$args ：这个变量等于请求行中的参数，同$query_string$content_length ： 请求头中的Content-length字段。$content_type ： 请求头中的Content-Type字段。$document_root ： 当前请求在root指令中指定的值。$host ： 请求主机头字段，否则为服务器名称。$http_user_agent ： 客户端agent信息$http_cookie ： 客户端cookie信息$limit_rate ： 这个变量可以限制连接速率。$request_method ： 客户端请求的动作，通常为GET或POST。$remote_addr ： 客户端的IP地址。$remote_port ： 客户端的端口。$remote_user ： 已经经过Auth Basic Module验证的用户名。$request_filename ： 当前请求的文件路径，由root或alias指令与URI请求生成。$scheme ： HTTP方法（如http，https）。$server_protocol ： 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。$server_addr ： 服务器地址，在完成一次系统调用后可以确定这个值。$server_name ： 服务器名称。$server_port ： 请求到达服务器的端口号。$request_uri ： 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。$uri ： 不带请求参数的当前URI，$uri不包含主机名，如”/foo/bar.html”。$document_uri ： 与$uri相同。 如： 访问链接是：http://localhost:88/test1/test2/test.php网站路径是：/var/www/html 123456$host：localhost$server_port：88$request_uri：http://localhost:88/test1/test2/test.php$document_uri：/test1/test2/test.php$document_root：/var/www/html$request_filename：/var/www/html/test1/test2/test.php 例子如果文件不存在则返回400 123if (!-f $request_filename) &#123; return 400;&#125; 如果host不是xuexb.com，则301到xuexb.com中 123if ( $host != \"xuexb.com\" )&#123; rewrite ^/(.*)$ https://xuexb.com/$1 permanent;&#125; 如果请求类型不是POST则返回405 123if ($request_method = POST) &#123; return 405;&#125; 如果参数中有 a=1 则301到指定域名 123if ($args ~ a=1) &#123; rewrite ^ http://example.com/ permanent;&#125; 在某种场景下可结合location规则来使用，如：访问 /test.html 时 123456789101112location = /test.html &#123; # 默认值为xiaowu set $name xiaowu; # 如果参数中有 name=xx 则使用该值 if ($args ~* name=(\\w+?)(&amp;|$)) &#123; set $name $1; &#125; # 301 rewrite ^ /$name.html permanent;&#125; 上面表示： /test.html =&gt; /xiaowu.html/test.html?name=ok =&gt; /ok.html?name=ok location语法在server块中使用，如： 1234server &#123; location 表达式 &#123; &#125;&#125; location表达式类型 如果直接写一个路径，则匹配该路径下的~ 表示执行一个正则匹配，区分大小写~* 表示执行一个正则匹配，不区分大小写^~ 表示普通字符匹配。使用前缀匹配。如果匹配成功，则不再匹配其他location。= 进行普通字符精确匹配。也就是完全匹配。优先级等号类型（=）的优先级最高。一旦匹配成功，则不再查找其他匹配项。^~类型表达式。一旦匹配成功，则不再查找其他匹配项。正则表达式类型（~ ~*）的优先级次之。如果有多个location的正则能匹配的话，则使用正则表达式最长的那个。常规字符串匹配类型。按前缀匹配。 例子 - 假地址掩饰真地址 123456789101112server &#123; # 用 xxoo_admin 来掩饰 admin location / &#123; # 使用break拿一旦匹配成功则忽略后续location rewrite /xxoo_admin /admin break; &#125; # 访问真实地址直接报没权限 location /admin &#123; return 403; &#125;&#125;","categories":[{"name":"nginx","slug":"nginx","permalink":"https://ngames-dev.cn/categories/nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://ngames-dev.cn/tags/nginx/"}]},{"title":"Git Page添加网易云音乐","slug":"Git Page添加网易云音乐","date":"2019-04-25T16:00:00.000Z","updated":"2019-09-06T09:23:30.124Z","comments":true,"path":"2019/04/26/Git Page添加网易云音乐/","link":"","permalink":"https://ngames-dev.cn/2019/04/26/Git Page添加网易云音乐/","excerpt":"添加到 _post.html 合适的位置 在写好的markdown文档的头文件中添加：music-id: xxx 配置项。具体的id号就是选取的歌曲的外链中的id号。 1&lt;iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&amp;id=&#123; &#123; page.music-id &#125; &#125;&amp;auto=1&amp;height=66\"&gt;&lt;/iframe&gt;","text":"添加到 _post.html 合适的位置 在写好的markdown文档的头文件中添加：music-id: xxx 配置项。具体的id号就是选取的歌曲的外链中的id号。 1&lt;iframe frameborder=\"no\" border=\"0\" marginwidth=\"0\" marginheight=\"0\" width=330 height=86 src=\"//music.163.com/outchain/player?type=2&amp;id=&#123; &#123; page.music-id &#125; &#125;&amp;auto=1&amp;height=66\"&gt;&lt;/iframe&gt; 但是这个好像被网易云屏蔽了 /assets/mp3/duli.mp3 _includes中添加文件open-embed.html， [文件内容](https://github.com/SakuraGaara/sakuragaara.github.io/blob/master/_includes/open-embed.html) 而后在_includes/_layout.html文件中添加 \\{\\% include open-embed.html \\%\\} ，以每次打开页面都能加载open-embed.html中的内容 最后添加音乐之需要在md文件中添加 1&lt;p&gt;http://xxxx.mp3&lt;/p&gt; 获取网易云音乐id http://music.163.com/song/media/outer/url?id=id.mp3","categories":[{"name":"网易云","slug":"网易云","permalink":"https://ngames-dev.cn/categories/网易云/"}],"tags":[{"name":"网易云","slug":"网易云","permalink":"https://ngames-dev.cn/tags/网易云/"}]},{"title":"一图掌握kubernetes客户端命令kubectl","slug":"一图掌握kubernetes客户端命令kubectl","date":"2019-04-25T16:00:00.000Z","updated":"2019-09-06T09:23:30.124Z","comments":true,"path":"2019/04/26/一图掌握kubernetes客户端命令kubectl/","link":"","permalink":"https://ngames-dev.cn/2019/04/26/一图掌握kubernetes客户端命令kubectl/","excerpt":"","text":"kubectl客户端命令 https://sakuragaara.github.io/images/img/20190426/kubernetes-kubectl-cheatsheet.png","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/tags/kubernetes/"},{"name":"kubectl","slug":"kubectl","permalink":"https://ngames-dev.cn/tags/kubectl/"}]},{"title":"kubernetes Pod控制器","slug":"kuberneres Pod控制器","date":"2019-04-22T16:00:00.000Z","updated":"2019-10-31T10:54:37.088Z","comments":true,"path":"2019/04/23/kuberneres Pod控制器/","link":"","permalink":"https://ngames-dev.cn/2019/04/23/kuberneres Pod控制器/","excerpt":"Pod是kubernetes的最小单元, 自主式创建的Pod删除之后就没有了，但是通过资源控制器创建的Pod如果被删除还会重建 自主式Pod： 创建一个Pod资源清单，kubectl create -f xxxx.yaml 资源控制器： kubectl run xxxx --image=xxxx --replicas=2 --port=80 默认属于 deployment控制器管理 Pod控制器 就是用于实现代替我们去管理pod的中间层，并帮助我们确保每一个Pod资源处于我们所定义或期望的目标状态pod资源出现故障首先要重启，如果一直重启有问题的话会基于某种策略重新编排。自动适应期望pod数量 Pod资源清单 作为模板内嵌在Pod控制器内进行创建","text":"Pod是kubernetes的最小单元, 自主式创建的Pod删除之后就没有了，但是通过资源控制器创建的Pod如果被删除还会重建 自主式Pod： 创建一个Pod资源清单，kubectl create -f xxxx.yaml 资源控制器： kubectl run xxxx --image=xxxx --replicas=2 --port=80 默认属于 deployment控制器管理 Pod控制器 就是用于实现代替我们去管理pod的中间层，并帮助我们确保每一个Pod资源处于我们所定义或期望的目标状态pod资源出现故障首先要重启，如果一直重启有问题的话会基于某种策略重新编排。自动适应期望pod数量 Pod资源清单 作为模板内嵌在Pod控制器内进行创建 Pod控制器ReplicaSetReplicaSet: 新一代的ReplicationController代用户创建指定数量的Pod副本数量，并确保Pod副本一直满足用户期望的数量状态，多退少补，而且还支持自动扩缩容机制但是kuberneters不建议直接使用ReporcaSetReporcaSet主要由三个重要组件组成：（1） 管控用户期望的Pod副本数量（2） 标签选择器，判定归自己管理可控制的Pod副本（3） Pod资源模板（当现存的pod副本数量不足，会根据Pod资源模板进行新建，帮助用户管理无状态的Pod资源，精确反应用户定义的目标数量） DeploymentDeployment： 无状态，守护进程类，只关注群体不关注个体工作在ReplicaSet之上，通过ReplicaSet管理无状态Pod资源，是目前来说最好的控制器(意味着满足ReplicaSet的所有功能)除此之外，支持滚动更新和回滚等机制，而且还提供声明式配置，可随时通过修改声明来定义目标期望状态 DaemonSetDaemonSet：无状态，守护进程类，只关注群体不关注个体 Pod与Node一对一的关系确保集群中每一个节点上只运行一个特定的Pod副本，系统级的后台任务，新增节点他都会自动添加pod也可以是满足条件的节点上运行特定的副本 JobJob：有状态，一次性任务只要完成就立即退出，不需要重启或重建，没有完成重构job。只能执行一次性任务 CronjobCronjob：有状态，周期性任务周期性任务控制，不需要持续后台运行 StatefulSetStatefulSet：管理有状态应用管理有状态应用（redis cluster）针对管理的应用器配置管理是不一样的，没有什么共通的规律，需要人为的封装在脚本中实行，相当之大的逻辑处理。（运维技能封装到运维脚本中） Pod控制器应用ReplicaSethelp:kubectl explain ReplicaSet 简写 rsReplicaSet先定义控制器 apiVersion,kind,metadata,spec等资源而在spec资源中，定义Pod数量，控制器标签选择器和Pod的模板模板中定义Podmetadata,spec等资源，metadata中Pod的标签必须需要继承控制器的标签属性支持动态修改kubectl edit rs myapp , 但是并不支持修改模板中的Pod内容，因为控制器定义的数量不变,除非人为手动删除控制器中的pod 123456789101112131415161718192021222324252627282930apiVersion: apps/v1kind: ReplicaSetmetadata: name: myapp namespace: defaultspec: replicas: 2 # Pod数量 selector: matchLabels: # 定义ReplicaSet标签 app: myapp template: metadata: name: flask-app namespace: default labels: # 必须继承ReplicaSet标签，否则一直创建 app: myapp ifram: flask spec: containers: - name: flaskapp image: sakuragaara/flaskapp:v1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 5000 livenessProbe: tcpSocket: port: 5000 timeoutSeconds: 3 restartPolicy: Always Deploymenthelp: kubectl explain deployment 简写 deployDeployment与ReplicaSet一样，定义控制器apiVersion,kind,metadata,spec等资源而在spec资源中，定义Pod数量，控制器标签选择器和Pod的模板支持动态修改Pod副本数量，更新资源镜像，支持回滚 修改Pod副本数量,更新资源Pod镜像 可以使用kubectl edit deployment flask-deploy编辑修改，也可以编辑文件flask-deploy.yaml之后，kubectl apply -f flask-deploy.yaml更新，默认为滚动更新方式 更新Pod镜像资源后，可以使用kubectl get rs -o wide查看,默认的rs模板被保留，这就是备份，随时可以使用它进行回滚 创建和查看 更新副本数量 可使用修改yaml更新，可以kubectl edit或者kubectl scale更新 更新镜像 使用修改yaml更新，也可以使用kubectl set image的方式进行更新 回滚 使用kubectl rollout history deployment flask-deploy,可查看Deployment更新历史版本,--revision=1 可查看Deployment的详细信息，在kubectl apply时添加--record才会查看到更新命令备注信息 使用kubectl rollout undo deployment flask-deploy --to-revision=1 进行回滚 使用修改补丁方式 如kubectl patch deployment flask-deploy -p &#39;{&quot;spec&quot;:{&quot;replicas&quot;:5}}&#39; -p参数，只支持json格式修改 灰度发布（金丝雀发布） Deployment默认为滚动更新方式，kubectl describe deployment flask-deploy 可以查看，可以修改其发布方式为灰度发布 spec.strategy.rollingUpdate.maxUnavailable 用来控制不可用Pod数量的最大值，从而在删除旧Pod时保证一定数量的可用Pod。当replicas=3，如果配置为1，则更新过程中会保证至少有2个可用Pod。默认为1 spec.strategy.rollingUpdate.maxSurge 用来控制超过期望数量的Pod数量最大值，从而在创建新Pod时限制总量。当replicas=3,如配置为1,则更新过着中会保证Pod总数量最多有4个。默认为1 （1）补丁方式更新为灰度发布 kubectl patch deploy flask-deploy -p &#39;{&quot;spec&quot;:{&quot;strategy&quot;:{&quot;rollingUpdate&quot;:{&quot;maxSurge&quot;:1,&quot;maxUnavailable&quot;:0}}}}&#39; （2）更新镜像，暂停,并监控pod更新状态 123456789101112131415161718192021222324252627282930313233cat flask-deploy.yamlapiVersion: apps/v1kind: Deploymentmetadata: name: flask-deploy namespace: defaultspec: replicas: 2 selector: matchLabels: app: flask template: metadata: name: flask-pod namespace: default labels: app: flask langure: python spec: containers: - name: flask-container image: sakuragaara/flaskapp:v1 imagePullPolicy: IfNotPresent ports: - name: http containerPort: 5000 livenessProbe: httpGet: port: 5000 scheme: HTTP path: /index initialDelaySeconds: 10 timeoutSeconds: 5","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/tags/kubernetes/"}]},{"title":"Flask蓝图","slug":"Flask蓝图","date":"2019-04-17T16:00:00.000Z","updated":"2019-10-14T07:39:31.007Z","comments":true,"path":"2019/04/18/Flask蓝图/","link":"","permalink":"https://ngames-dev.cn/2019/04/18/Flask蓝图/","excerpt":"Flask蓝图提供了模块化管理程序路由的功能，使程序结构清晰、简单易懂首先，让我们来看下flask应用与蓝图之间的关系。蓝图代表着应用的一个个功能模块，能做到即插即用。 结构1234567project├── app│ ├── __init__.py│ └── web│ ├── __init__.py│ └── login.py└── app.py","text":"Flask蓝图提供了模块化管理程序路由的功能，使程序结构清晰、简单易懂首先，让我们来看下flask应用与蓝图之间的关系。蓝图代表着应用的一个个功能模块，能做到即插即用。 结构1234567project├── app│ ├── __init__.py│ └── web│ ├── __init__.py│ └── login.py└── app.py Demo Code app/__init__.py 12345678910111213from flask import Flaskdef create_app(): app = Flask(__name__) app.secret_key = 'XxxxxxX' registry_blueprint(app) return appdef registry_blueprint(app): from app.web import web app.register_blueprint(web) app.py 12345from app import create_appapp = create_app()if __name__ == '__main__': app.run() web/__init__.py 12345from flask import Blueprintweb = Blueprint('web', __name__)from app.web import login web/login.py 123456from . import web@web.route('/login/&lt;username&gt;')def login(username): return username","categories":[{"name":"Flask","slug":"Flask","permalink":"https://ngames-dev.cn/categories/Flask/"}],"tags":[{"name":"Flask","slug":"Flask","permalink":"https://ngames-dev.cn/tags/Flask/"}]},{"title":"kubernetes Pod资源清单注解","slug":"kubernetes Pod资源清单注解","date":"2019-04-17T16:00:00.000Z","updated":"2019-11-01T10:13:36.177Z","comments":true,"path":"2019/04/18/kubernetes Pod资源清单注解/","link":"","permalink":"https://ngames-dev.cn/2019/04/18/kubernetes Pod资源清单注解/","excerpt":"创建资源的方法：定义yaml格式提供配置清单,将资源清单提交给apiServer apiServer可自动将其转换为json格式，而后提交给Scheduler(集群中的调度器) 由Scheduler完成调度，调度目标节点完成创建，并启动相关服务 Pod核心资源配置清单： 资源清单定义帮助 kubectl explain kubectl explain pods 等查看可嵌套字段 apiVersion 格式为group/version，所属群组版本支持的版本 kubectl api-versions 可查看 kind 定义资源类别，Pod,Service,Deployment,Event,Secret等(注意大小写)","text":"创建资源的方法：定义yaml格式提供配置清单,将资源清单提交给apiServer apiServer可自动将其转换为json格式，而后提交给Scheduler(集群中的调度器) 由Scheduler完成调度，调度目标节点完成创建，并启动相关服务 Pod核心资源配置清单： 资源清单定义帮助 kubectl explain kubectl explain pods 等查看可嵌套字段 apiVersion 格式为group/version，所属群组版本支持的版本 kubectl api-versions 可查看 kind 定义资源类别，Pod,Service,Deployment,Event,Secret等(注意大小写) metadata 元数据 kubectl explain pods.metadata 查看帮助 123456metadata: name: pod_name namespace: 名称空间 labels: -- 标签 key: value annotations: 资源注解 spec disired state 期望状态, kubectl explain pods.spec查看帮助 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465spec: containers: - name: &lt;string&gt; image: &lt;string&gt; imagePullPolicy: &lt;string&gt; [IfNotPresent,Always,Never] # Always：不管镜像是否存在都会进行一次拉取。 # Never：不管镜像是否存在都不会进行拉取 # IfNotPresent：只有镜像不存在时，才会进行镜像拉取,默认为IfNotPresent，但:latest标签的镜像默认为Always ports: &lt;[]Object&gt; - name: &lt;string&gt; containerPort: &lt;integer&gt; command: &lt;[]string&gt; # 运行的应用程序，类似docker的entrypoint,并且这里的命令不会允许中shell中 args: &lt;[]string&gt; # args将参数传给command lifecycle: postStart: &lt;Object&gt; # 容器创建成功后，运行前的任务，用于资源部署、环境准备等，在完成之前，容器处于ContainerCreating状态 preStop: &lt;Object&gt; # 在容器被终止前的任务，用于优雅关闭应用程序、通知其他系统等等 livenessProbe: &lt;Object&gt; # 存活探针,确定何时重启容器当应用程序处于运行状态但无法做进一步操作， # liveness探针将捕获到deadlock，重启处于该状态下的容器 # Kubernetes支持3种类型的应用健康检查动作，分别为HTTP Get、Container Exec和TCP Socket readinessProbe: &lt;Object&gt; # 就绪探针,确定容器是否已经就绪可以接受流量, # 只有当Pod中的容器都处于就绪状态时kubelet才会认定该Pod处于就绪状态 # 就绪状态, pod才会按照标签加入service ########### livenessProbe,readinessProbe 中接受属性 exec: &lt;Object&gt; command: &lt;[]string&gt; httpGet: &lt;Object&gt; host: &lt;string&gt; # 连接的主机名，默认连接到pod的IP。你可能想在http header中设置”Host”而不是使用IP httpHeaders: &lt;[]Object&gt; # 自定义请求的header。HTTP运行重复的header path: &lt;string&gt; # 访问的HTTP server的path port: &lt;string&gt; # 访问的容器的端口名字或者端口号。端口号必须介于1和65525之间 scheme: &lt;string&gt; # 连接使用的schema，默认HTTP tcpSocket: &lt;Object&gt; host: &lt;string&gt; port: &lt;string&gt; initialDelaySeconds: &lt;integer&gt; # 容器启动后，等待多少秒之后进行第一次探测 periodSeconds: &lt;integer&gt; # 执行探测的频率。默认是10秒，最小1秒 successThreshold: &lt;integer&gt; # 探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。对于liveness必须是1。最小值是1 failureThreshold: &lt;integer&gt; # 探测成功后，最少连续探测失败多少次才被认定为失败。默认是3。最小值是1 timeoutSeconds: &lt;integer&gt; # 探测超时时间。默认1秒，最小1秒 resources: # CPU的单位是milicpu，500mcpu=0.5cpu；而内存的单位则包括E, P, T, G, M, K, Ei, Pi, Ti, Gi, Mi, Ki等 requests: # 请求 cpu: \"300m\" memory: \"64Mi\" limits: # 上限 cpu: \"500m\" memory: \"128Mi\" initContainers: # Init Container在所有容器运行之前执行（run-to-completion），常用来初始化配置 - name: &lt;string&gt; nodeName: nodename 指定node节点 nodeSelector: &lt;map[string]string&gt; 指定node的label标签 restartPolicy: &lt;string&gt; Always, OnFailure, Never. Default to Always. # Always：只要退出就重启 # OnFailure：失败退出（exit code不等于0）时重启 # Never：只要退出就不再重启 status 当前状态 current state(只读), 由kubernetes集群维护,不需要用户自己定义 Demo Code12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152apiVersion: v1kind: Podmetadata: name: flaskapp-pod-v1 namespace: default labels: app: flaskapp version: v1 annotations: note: \"This is Flask app.\"spec: containers: - name: flaskapp-container-v1 image: sakuragaara/flaskapp:v1 imagePullPolicy: Never ports: - name: http containerPort: 5000 livenessProbe: httpGet: port: 5000 path: /index initialDelaySeconds: 30 periodSeconds: 5 successThreshold: 1 timeoutSeconds: 8 env: - name: MY_NODE_NAME valueFrom: fieldRef: fieldPath: spec.nodeName - name: MY_POD_NAME valueFrom: fieldRef: fieldPath: metadata.name - name: MY_POD_NAMESPACE valueFrom: fieldRef: fieldPath: metadata.namespace - name: MY_POD_IP valueFrom: fieldRef: fieldPath: status.podIP - name: MY_APP_NAME value: Flask APP resources: requests: cpu: \"0.1\" memory: \"56Mi\" limits: cpu: \"1\" memory: \"128Mi\" tcpSocket没有加host，主要是host默认为containers的IP，而flask启动是0.0.0.0，使用127.0.0.1监听会报错","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/tags/kubernetes/"}]},{"title":"AWS云搭建安全简单免费的VPN服务","slug":"AWS云搭建安全简单免费的VPN服务","date":"2019-04-15T16:00:00.000Z","updated":"2019-10-14T07:39:31.006Z","comments":true,"path":"2019/04/16/AWS云搭建安全简单免费的VPN服务/","link":"","permalink":"https://ngames-dev.cn/2019/04/16/AWS云搭建安全简单免费的VPN服务/","excerpt":"Reference resources:https://www.webdigi.co.uk/blog/2015/how-to-setup-your-own-private-secure-free-vpn-on-the-amazon-aws-cloud-in-10-minutes/ Creating your Private VPN Server================================ 1. Setup a free Amazon (AWS) cloud account.Visit http://aws.amazon.com/free/ and complete the signup. If you already have an Amazon AWS account then please login and follow on.","text":"Reference resources:https://www.webdigi.co.uk/blog/2015/how-to-setup-your-own-private-secure-free-vpn-on-the-amazon-aws-cloud-in-10-minutes/ Creating your Private VPN Server================================ 1. Setup a free Amazon (AWS) cloud account.Visit http://aws.amazon.com/free/ and complete the signup. If you already have an Amazon AWS account then please login and follow on. Select a region for your VPN server.The VPN server can be in the following locations – North Virginia, Oregon, California, Ireland, Frankfurt, Singapore, Tokyo, Sydney, São Paulo. All your traffic will flow through the region that your VPN server is hosted. The selected region will appear in bold next to your name on the top header bar. 3. Open CloudFormation in the Amazon AWS control panel.You can follow this link or click on the cloud formation link from the AWS page. 4. Start creating a stack with CloudFormation. Click on “Create Stack” button on top of the page. 5. Setting up the template for the stackEnter a stack Name say MyVPN (you use what you like). Then under Template, Source, select “Specify an Amazon S3 template URL” and paste in this URL https://s3.amazonaws.com/webdigi/VPN/Unified-Cloud-Formation.yaml and then click Next. 6. Setup VPN access details in the Specify Parameters pageSpeed: Select Standard.VPN-Free and this should do for most use cases. We have also added faster server options if you ever require VPN with multiple simultaneous video streams and so on.Username: VPN username for your VPN server.VPNPassword: VPN password for your VPN server.VPNPhrase: VPN passphrase the L2TP – IPSEC connections on your VPN server. 7. You will then be taken to the Options section and you can click Next without having to fill anything on this page. Finally, you will see a review page as in the screenshot below. Just click on Create and the VPN server will be created in a few minutes. 8. Monitoring the VPN server creation You will see a page which shows that the status is Create in progress as below. Within about 2 minutes you should see that the stack create in progress is complete as below. 9. Obtain the private VPN server IP address Once the stack status shows as CREATE_COMPLETE you can then click on the Outputs tab. Now in the outputs tab you can see the server IP address as highlighted below. Awesome, you should now have your private VPN server running in the IP address shown in the outputs tab. Please note that the IP address is unique for your server and you need it to connect your devices. Now your VPN server is ready and let us connect to it. Connecting to your private VPN server===================================== Each device has its own configuration to connect to a VPN server. We have added a how to for a few popular devices below. Please note that your private VPN server supports both PPTP and L2TP with IPSEC. This means that your VPN server supports most devices out there including older routers. You can connect to your VPN server with either PPTP or L2TP as supported by your device. The parameters for your VPN connection areServer Address: The IP address from step 9 and this is unique for your VPN server.VPN Username &amp; Password: From step 6 above. Same username &amp; password for PPTP / L2TP VPN.VPN Passphrase: You set this up on step 6 above and only have to be used with an L2TP connection. Examples below use PPTP but you can also find out how to setup L2TP with IPSEC on various websites. 1. Setting up VPN on an Android 5.0 2. Setting up VPN on a MAC with the PPTP connection.*UPDATE Nov 2016: PPTP is not supported on macOS Sierra so follow point 3 using L2TP over IPSEC (below).*First open System Preferences, then Network and follow the screenshots below. 3. Setting up VPN on a Mac with L2TP over IPSecUPDATE Nov 2016: L2TP is recommended now. Follow instructions as in PPTP after you setup up to point 8 below. Follow other steps as outlined in the above section on PPTP to finish the setup. 4. Setting up VPN on Asus RT-AC68U router 5. For all other devices please search for Setting up PPTP VPN on my iphone and so on. You can also setup an L2TP IPSEC VPN which is more secure but might not be supported on all devices. Tips / Suggestions================== 1. If you want to delete your VPN server then just open CloudFormation on AWS. Make sure you select the same region that you created your VPN server. Then just click on Delete Stack button and your private VPN server will be removed. 2. You can have multiple VPN servers all over the world. You just have to repeat the setup steps in this guide by selecting different regions. Please note that AWS free tier gives you a total of 750 hours a month free. You can also delete and create VPN servers as frequently as you want. 3. Setting up a VPN connection on your router will allow all devices on its network to use the VPN server. This could be beneficial for use with AppleTV / Chromecast and any device that does not support a VPN. 4. You can test if your VPN connection is active by just searching for “what is my ip address” on your favourite search engine. The IP address reported will be that of your private VPN server if everything is your connection is enabled. If your VPN connection is not enabled or if the VPN server settings are not complete then it will report your ISP’s IP address. 5. We love your feedback and let us know if you face any issues in the comments section below or on our github page for setting up your private VPN on AWS .","categories":[{"name":"VPN","slug":"VPN","permalink":"https://ngames-dev.cn/categories/VPN/"}],"tags":[{"name":"vpn","slug":"vpn","permalink":"https://ngames-dev.cn/tags/vpn/"}]},{"title":"Nginx访问控制(持续更新)","slug":"Nginx访问控制(持续更新)","date":"2019-04-09T16:00:00.000Z","updated":"2019-10-14T07:39:31.006Z","comments":true,"path":"2019/04/10/Nginx访问控制(持续更新)/","link":"","permalink":"https://ngames-dev.cn/2019/04/10/Nginx访问控制(持续更新)/","excerpt":"NGINX实现IF语句里的AND，OR多重判断正常清空下使用allow,deny 可以完成，但是对于同时过滤ip和页面，就无法按照正常规则去实现 Nginx的rewrite规则参考 ~ 为区分大小写匹配 ~* 为不区分大小写匹配 !和!*分别为区分大小写不匹配及不区分大小写不匹","text":"NGINX实现IF语句里的AND，OR多重判断正常清空下使用allow,deny 可以完成，但是对于同时过滤ip和页面，就无法按照正常规则去实现 Nginx的rewrite规则参考 ~ 为区分大小写匹配 ~* 为不区分大小写匹配 !和!*分别为区分大小写不匹配及不区分大小写不匹 -f和!-f用来判断是否存在文件 -d和!-d用来判断是否存在目录 -e和!-e用来判断是否存在文件或目录 -x和!-x用来判断文件是否可执行 last 相当于Apache里的[L]标记，表示完成rewrite，呵呵这应该是最常用的 break 终止匹配, 不再匹配后面的规则 redirect 返回302临时重定向 地址栏会显示跳转后的地址 permanent 返回301永久重定向 地址栏会显示跳转后的地址 Nginx参数 解释 arg_PARAMETER 这个变量包含GET请求中，如果有变量PARAMETER时的值。 args 这个变量等于请求行中(GET请求)的参数，如：foo=123&amp;bar=blahblah; binary_remote_addr 二进制的客户地址。 body_bytes_sent 响应时送出的body字节数数量。即使连接中断，这个数据也是精确的。 content_length 请求头中的Content-length字段。 content_type 请求头中的Content-Type字段。 cookie_COOKIE cookie COOKIE变量的值 document_root 当前请求在root指令中指定的值。 document_uri 与uri相同。 host 请求主机头字段，否则为服务器名称。 hostname Set to themachine’s hostname as returned by gethostname http_HEADER is_args 如果有args参数，这个变量等于”?”，否则等于””，空值。 http_user_agent 客户端agent信息 http_cookie 客户端cookie信息 limit_rate 这个变量可以限制连接速率。 query_string 与args相同。 request_body_file 客户端请求主体信息的临时文件名。 request_method 客户端请求的动作，通常为GET或POST。 remote_addr 客户端的IP地址。 remote_port 客户端的端口。 remote_user 已经经过Auth Basic Module验证的用户名。 request_completion 如果请求结束，设置为OK. 当请求未结束或如果该请求不是请求链串的最后一个时，为空(Empty)。 request_method GET或POST request_filename 当前请求的文件路径，由root或alias指令与URI请求生成。 request_uri 包含请求参数的原始URI，不包含主机名，如：”/foo/bar.php?arg=baz”。不能修改。 scheme HTTP方法（如http，https）。 server_protocol 请求使用的协议，通常是HTTP/1.0或HTTP/1.1。 server_addr 服务器地址，在完成一次系统调用后可以确定这个值。 server_name 服务器名称。 server_port 请求到达服务器的端口号。 NGINX实现IF语句里的AND，OR多重判断nginx的配置中不支持if条件的逻辑与／逻辑或运算 ，并且不支持if的嵌套语法，我们可以用变量的方式来实现具体方法为AND 就用变量叠加，OR就用0或1切换 如定义结尾以swagger-ui.html和api-docs的页面，只能由202.101.172.35和8.8.8.8访问： 伪代码 123if ($remote_addr !~ &quot;(202.101.172.35|8.8.8.8)&quot; &amp;&amp; $request_uri ~* (swagger-ui.html|api-docs)$) &#123; return 403;&#125; 上述伪代码Nginx实现 123456789101112location / &#123; set $flag 0; if ($remote_addr !~ &quot;122.224.128.14&quot;) &#123; set $flag &quot;$&#123;flag&#125;1&quot;; &#125; if ($request_uri ~* (swagger-ui.html|api-docs)$) &#123; set $flag &quot;$&#123;flag&#125;2&quot;; &#125; if ($flag = &quot;012&quot;) &#123; return 403; &#125;&#125;","categories":[{"name":"Nginx","slug":"Nginx","permalink":"https://ngames-dev.cn/categories/Nginx/"}],"tags":[{"name":"nginx","slug":"nginx","permalink":"https://ngames-dev.cn/tags/nginx/"}]},{"title":"阿里云ECS安全基线检查","slug":"阿里云ECS安全基线检查","date":"2019-04-08T16:00:00.000Z","updated":"2019-10-14T07:39:31.005Z","comments":true,"path":"2019/04/09/阿里云ECS安全基线检查/","link":"","permalink":"https://ngames-dev.cn/2019/04/09/阿里云ECS安全基线检查/","excerpt":"阿里云ECS安全基线检查Centos6, Centos7 文件权限1, 设置用户权限配置文件的权限描述设置用户权限配置文件的权限加固建议执行以下5条命令 12345chown root:root /etc/passwd /etc/shadow /etc/group /etc/gshadowchmod 0644 /etc/group chmod 0644 /etc/passwd chmod 0400 /etc/shadow chmod 0400 /etc/gshadow","text":"阿里云ECS安全基线检查Centos6, Centos7 文件权限1, 设置用户权限配置文件的权限描述设置用户权限配置文件的权限加固建议执行以下5条命令 12345chown root:root /etc/passwd /etc/shadow /etc/group /etc/gshadowchmod 0644 /etc/group chmod 0644 /etc/passwd chmod 0400 /etc/shadow chmod 0400 /etc/gshadow /etc/ssh/sshd_config2，确保SSH MaxAuthTries设置为3到6之间描述设置较低的Max AuthTrimes参数将降低SSH服务器被暴力攻击成功的风险加固建议在 /etc/ssh/sshd_config 中取消MaxAuthTries注释符号#，设置最大密码尝试失败次数3-6，建议为4： MaxAuthTries 4 3, 禁止SSH空密码用户登录描述禁止SSH空密码用户登录加固建议在 /etc/ssh/sshd_config 中取消PermitEmptyPasswords no注释符号# 4, 确保SSH LogLevel设置为INFO描述确保SSH LogLevel设置为INFO,记录登录和注销活动加固建议编辑 /etc/ssh/sshd_config 文件以按如下方式设置参数(取消注释): LogLevel INFO 5, SSHD强制使用V2安全协议描述SSHD强制使用V2安全协议加固建议编辑 /etc/ssh/sshd_config 文件以按如下方式设置参数： Protocol 2 6, 设置SSH空闲超时退出时间描述设置SSH空闲超时退出时间,可降低未授权用户访问其他用户ssh会话的风险加固建议编辑/etc/ssh/sshd_config，将ClientAliveInterval 设置为300到900，即5-15分钟，将ClientAliveCountMax设置为0。 ClientAliveInterval 900 ClientAliveCountMax 0 /etc/login.defs7, 设置密码失效时间描述设置密码失效时间，强制定期修改密码，减少密码被泄漏和猜测风险，使用非密码登陆方式(如密钥对)请忽略此项加固建议使用非密码登陆方式如密钥对，请忽略此项。在 /etc/login.defs 中将 PASS_MAX_DAYS 参数设置为 60-180之间，如 PASS_MAX_DAYS 90 。需同时执行命令设置root密码失效时间： chage --maxdays 90 root 8, 设置密码修改最小间隔时间描述设置密码修改最小间隔时间，限制密码更改过于频繁加固建议在 /etc/login.defs 中将 PASS_MIN_DAYS 参数设置为7-14之间,建议为7： PASS_MIN_DAYS 7 需同时执行命令为root用户设置： chage --mindays 7 root 9, 确保密码到期警告天数为7或更多描述确保密码到期警告天数为7或更多加固建议在 /etc/login.defs 中将 PASS_WARN_AGE 参数设置为7-14之间，建议为7： PASS_WARN_AGE 7 。同时执行命令使root用户设置生效： chage --warndays 7 root","categories":[{"name":"安全","slug":"安全","permalink":"https://ngames-dev.cn/categories/安全/"}],"tags":[{"name":"ECS安全","slug":"ECS安全","permalink":"https://ngames-dev.cn/tags/ECS安全/"}]},{"title":"mysqlbinlog实时同步至elasticsearch","slug":"mysqlbinlog实时同步至elasticsearch","date":"2019-04-01T16:00:00.000Z","updated":"2019-10-14T07:39:31.005Z","comments":true,"path":"2019/04/02/mysqlbinlog实时同步至elasticsearch/","link":"","permalink":"https://ngames-dev.cn/2019/04/02/mysqlbinlog实时同步至elasticsearch/","excerpt":"canal从mysql中获取binlog日志信息，输出至kafka，logstash从kafka中获取日志信息，写入elasticsearch不过看起来好像毫无意义,所以做的比较简易","text":"canal从mysql中获取binlog日志信息，输出至kafka，logstash从kafka中获取日志信息，写入elasticsearch不过看起来好像毫无意义,所以做的比较简易 demo环境介绍 MYSQL: 使用docker做的一个5.7的环境，用于做自己网站的一个数据库 Canal: 是阿里巴巴旗下的一款开源项目，纯Java开发。基于数据库增量日志解析，提供增量数据订阅&amp;消费，目前主要支持了MySQL（也支持mariaDB） kafka: 使用docker-compose创建的一个集群[docker-compose.yml] elasticsearch+kibana: 使用docker-compose创建的单节点demo logstash: 用于从kafka写入elasticearch Canal简单配置1.修改以下文件 123vim conf/canal.propertiescanal.serverMode = kafka # 配置输出至kafkacanal.mq.servers = 127.0.0.1:9091 # kafka server 123456789vim conf/example/instance.propertiescanal.instance.master.address=127.0.0.1:3000 # mysql数据库地址canal.instance.dbUsername=canalcanal.instance.dbPassword=canalcanal.instance.connectionCharset = UTF-8canal.instance.defaultDatabaseName = book # 可注释掉，则为所有数据库canal.mq.topic=book # 定义kafka订阅主题 2.启动canal 1./bin/startup.sh 3.以上则完成Canal配置，检查kafka是否可接受到canal传来的日志内容 123456789101112131415# -*- coding: utf-8 -*-from kafka import KafkaConsumerconsumer = KafkaConsumer('book',bootstrap_servers='127.0.0.1:9092', auto_offset_reset='earliest')for msg in consumer: print('topic: %s \\n partition: %s \\n offset: %s \\n headers: %s \\n timestamp: %s \\n timestamp_type: %s \\n key: %s \\n value: %s ' % ( msg.topic, msg.partition, msg.offset, msg.headers, msg.timestamp, msg.timestamp_type, msg.key, msg.value.decode('utf8')) ) 向数据库中插入数据，运行脚步是否能得到输出结果 ![mysqltest](/images/img/2019-04-02 2.16.59.png)![kafka-consumer](/images/img/2019-04-02 2.39.41.png) kafka既然能接受到canal传来的日志，接下来就可以配置logstash从Kafka接受消息写入es 配置logstash1.准备logstash配置文件kafka-logstash-es.conf 123456789101112131415161718input &#123; kafka &#123; bootstrap_servers =&gt; &quot;172.16.149.242:9092&quot; # 5.x版本，写法bootstrap_servers group_id =&gt; &quot;elastic_consumer&quot; topics =&gt; [&quot;book&quot;] consumer_threads =&gt; 3 decorate_events =&gt; true codec =&gt; &quot;json&quot; &#125;&#125;output &#123; elasticsearch &#123; hosts=&gt; [&quot;172.16.149.242:9200&quot;] index =&gt; &quot;logstash-book-%&#123;[table]&#125;-%&#123;+YYYY-MM-dd&#125;&quot; # 按照不同的表和日期做索引 codec =&gt; &quot;json&quot; &#125;&#125; logstash-kafka-plugin 配置文件参考配置与logstash版本相关 2.启动logstash,并查看es是否有索引 1./bin/logstash -f kafka-logstash-es.conf 3.验证es是否有索引和相关数据![check_es](/images/img/2019-04-02 3.06.28.png)","categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ngames-dev.cn/categories/elasticsearch/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ngames-dev.cn/tags/elasticsearch/"},{"name":"canal","slug":"canal","permalink":"https://ngames-dev.cn/tags/canal/"}]},{"title":"kubernetes应用[一]","slug":"kubernetes应用[一]","date":"2019-03-27T16:00:00.000Z","updated":"2019-10-14T07:39:31.004Z","comments":true,"path":"2019/03/28/kubernetes应用[一]/","link":"","permalink":"https://ngames-dev.cn/2019/03/28/kubernetes应用[一]/","excerpt":"kubectl相关命令操作 查看节点详细信息1[root@master-01 ~]# kubectl describe node node-01 pod创建 创建pod kubectl run 123456[root@master-01 ~]# kubectl run nginx-deploy --image=nginx:latest --port=80 --replicas=1 --dry-run=true nginx-deploy 创建pod名称 --image 指定镜像 --port 指定端口 --reolices 指定pod内创建几个容器 --dry-run 干跑模式","text":"kubectl相关命令操作 查看节点详细信息1[root@master-01 ~]# kubectl describe node node-01 pod创建 创建pod kubectl run 123456[root@master-01 ~]# kubectl run nginx-deploy --image=nginx:latest --port=80 --replicas=1 --dry-run=true nginx-deploy 创建pod名称 --image 指定镜像 --port 指定端口 --reolices 指定pod内创建几个容器 --dry-run 干跑模式 查看 查看pod kubectl get pods 123[root@master-01 ~]# kubectl get pods NAME READY STATUS RESTARTS AGE nginx-deploy-86bf78c77-ftscg 1/1 Running 0 4h17m 查看pod详细信息 kubectl describe 1[root@master-01 ~]# kubectl describe pod nginx-deploy-86bf78c77-ftscg 删除 删除pod kubectl delete 12[root@master-01 ~]# kubectl delete pod nginx-deploy-86bf78c77-ftscg pod &quot;nginx-deploy-86bf78c77-ftscg&quot; deleted 删除之后发现会自动创建一个pod[root@master-01 ~]# kubectl get pod 12NAME READY STATUS RESTARTS AGE nginx-deploy-86bf78c77-ffj8d 1/1 Running 0 35s 彻底删除 kubectl delete deployment12[root@master-01 ~]# kubectl delete deployment nginx-deploydeployment.extensions &quot;nginx-deploy&quot; deleted service创建 创建服务 kubectl expose 123456[root@master-01 ~]# kubectl expose deployment nginx-deploy --name=nginx --port=80 --target-port=80 --protocol=TCP nginx-deploy 为上面创建的pod名 --name 指定服务名 --port 指定pod端口 --target-port 指定service端口映射pod端口 --protocol 端口类型 查看 查看服务 kubectl get service 12345[root@master-01 ~]# kubectl get service NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 &lt;none&gt; 443/TCP 21h nginx ClusterIP 10.104.62.105 &lt;none&gt; 80/TCP 2m35s 这时，在pod内可以使用地址&lt;http://10.104.62.105&gt;或者服务名&lt;http://nginx&gt;访问nginx,但是节点依旧无法访问 查看服务具体信息 kubectl describe service 12345678910111213[root@master-01 ~]# kubectl describe service nginx Name: nginx Namespace: default Labels: run=nginx-deploy Annotations: &lt;none&gt; Selector: run=nginx-deploy Type: ClusterIP IP: 10.104.62.105 Port: &lt;unset&gt; 80/TCP TargetPort: 80/TCP Endpoints: 10.244.1.3:80 Session Affinity: None Events: &lt;none&gt; update更改pod规模 动态更新pod规模，增加/减少pod kubectl scale 123456789[root@master-01 ~]# kubectl scale --replicas=5 deployment nginx-deploy [root@master-01 ~]# kubectl get pod -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE client 1/1 Running 0 26m 10.244.2.3 node-02 &lt;none&gt; nginx-deploy-86bf78c77-8mp2p 1/1 Running 0 57s 10.244.2.5 node-02 &lt;none&gt; nginx-deploy-86bf78c77-ffj8d 1/1 Running 0 51m 10.244.1.3 node-01 &lt;none&gt; nginx-deploy-86bf78c77-g7s7j 1/1 Running 0 57s 10.244.2.6 node-02 &lt;none&gt; nginx-deploy-86bf78c77-tsxmp 1/1 Running 0 57s 10.244.1.4 node-01 &lt;none&gt; nginx-deploy-86bf78c77-vddwx 1/1 Running 0 57s 10.244.2.4 node-02 &lt;none&gt; 更改Pod镜像 滚动更新image kubectl set image 首先查看一下当前的 1234567891011[root@master-01 ~]# kubectl describe pod nginx-deploy|grep -E &quot;^Name:|Image:&quot; Name: nginx-deploy-86bf78c77-5h4rh Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-ffj8d Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-qn7zh Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-tkmtj Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-vddwx Image: nginx:1.14-alpine 使用 kubectl set image 更新镜像 12[root@master-01 ~]# kubectl set image deployment nginx-deploy nginx-deploy=nginx:1.15.10-perl deployment.extensions/nginx-deploy image updated 更新的同时可使用kubectl rollout status查看镜像升级状态 12345678910111213141516171819202122232425262728293031[root@master-01 ~]# kubectl rollout status deployment nginx-deploy Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 3 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 3 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 3 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 3 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 3 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 4 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 4 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 4 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 4 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 4 out of 5 new replicas have been updated... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 2 old replicas are pending termination... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 2 old replicas are pending termination... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 2 old replicas are pending termination... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 old replicas are pending termination... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 old replicas are pending termination... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 1 old replicas are pending termination... Waiting for deployment &quot;nginx-deploy&quot; rollout to finish: 4 of 5 updated replicas are available... deployment &quot;nginx-deploy&quot; successfully rolled out 再次查看Pod信息，镜像已变动 [root@master-01 ~]# kubectl describe pod nginx-deploy|grep -E &quot;^Name:|Image:&quot; Name: nginx-deploy-7596d8b647-58xcs Image: nginx:1.15.10-perl Name: nginx-deploy-7596d8b647-dlf76 Image: nginx:1.15.10-perl Name: nginx-deploy-7596d8b647-shkcn Image: nginx:1.15.10-perl Name: nginx-deploy-7596d8b647-tttkd Image: nginx:1.15.10-perl Name: nginx-deploy-7596d8b647-znf24 Image: nginx:1.15.10-perl 回滚镜像 回滚 kubectl rollout undo 123456789101112131415161718192021222324252627282930[root@master-01 ~]# kubectl rollout undo deployment nginx-deploy deployment.extensions/nginx-deploy [root@master-01 ~]# kubectl describe pod nginx-deploy|grep -E &quot;^Name:|Image:&quot; Name: nginx-deploy-7596d8b647-dlf76 Image: nginx:1.15.10-perl Name: nginx-deploy-7596d8b647-tttkd Image: nginx:1.15.10-perl Name: nginx-deploy-7596d8b647-znf24 Image: nginx:1.15.10-perl Name: nginx-deploy-86bf78c77-55bdr Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-9q54l Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-h6qlf Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-h9fgd Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-vfdzs Image: nginx:1.14-alpine [root@master-01 ~]# kubectl describe pod nginx-deploy|grep -E &quot;^Name:|Image:&quot; Name: nginx-deploy-86bf78c77-55bdr Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-9q54l Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-h6qlf Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-h9fgd Image: nginx:1.14-alpine Name: nginx-deploy-86bf78c77-vfdzs Image: nginx:1.14-alpine 设置外部访问 首先查看服务 123[root@master-01 ~]# kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx ClusterIP 10.104.62.105 &lt;none&gt; 80/TCP 5d23h 更改服务clusterIP类型kubectl edit svc 123456789101112131415161718192021222324[root@master-01 ~]# kubectl edit svc nginx apiVersion: v1 kind: Service metadata: creationTimestamp: 2019-03-22T07:50:32Z labels: run: nginx-deploy name: nginx namespace: default resourceVersion: &quot;98592&quot; selfLink: /api/v1/namespaces/default/services/nginx uid: 2b765b06-4c77-11e9-a182-000c29bb0a84 spec: clusterIP: 10.104.62.105 ports: port: 80 protocol: TCP targetPort: 80 selector: run: nginx-deploy sessionAffinity: None type: NodePorts status: loadBalancer: &#123;&#125; 再次查看服务 123[root@master-01 ~]# kubectl get svc nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx NodePort 10.104.62.105 &lt;none&gt; 80:32660/TCP 5d23h 此时可以使用集群内任意Node节点通过32660端口访问","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/tags/kubernetes/"}]},{"title":"python之elasticsearch初试","slug":"python之elasticsearch初试","date":"2019-03-26T16:00:00.000Z","updated":"2019-09-06T09:23:30.120Z","comments":true,"path":"2019/03/27/python之elasticsearch初试/","link":"","permalink":"https://ngames-dev.cn/2019/03/27/python之elasticsearch初试/","excerpt":"python elasticsearch库应用Doc: https://pypi.org/project/elasticsearch/","text":"python elasticsearch库应用Doc: https://pypi.org/project/elasticsearch/ 123456789101112131415161718192021222324252627282930313233343536373839404142# --coding: utf-8 --from elasticsearch import Elasticsearchdef get_es_config(): es = Elasticsearch( ['http://xxxxxxxxxxx.public.elasticsearch.aliyuncs.com'], http_auth=('elastic', '123456'), ) return esdef search_body(index, doc): \"\"\"搜索\"\"\" es=get_es_config() if es.indices.exists(index=index): res=es.search(index=index,body=doc) for line in res[\"hits\"][\"hits\"]: print(line)def close_index(index): \"\"\"关闭/删除索引\"\"\" es = get_es_config() if es.indices.exists(index=index): if es.indices.close(index): print(True) if es.indices.delete(index): print(True)search_body(\"my_index-2018.01.30\", doc=&#123; \"query\": &#123; \"match\": &#123; \"geoip.city_name\": \"Hangzhou\" &#125; &#125; &#125; )close_index(\"my_index-2018.01.30\")","categories":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ngames-dev.cn/categories/elasticsearch/"}],"tags":[{"name":"elasticsearch","slug":"elasticsearch","permalink":"https://ngames-dev.cn/tags/elasticsearch/"},{"name":"python","slug":"python","permalink":"https://ngames-dev.cn/tags/python/"}]},{"title":"kubernetes安装","slug":"kubernetes安装","date":"2019-03-19T16:00:00.000Z","updated":"2019-10-14T07:39:31.003Z","comments":true,"path":"2019/03/20/kubernetes安装/","link":"","permalink":"https://ngames-dev.cn/2019/03/20/kubernetes安装/","excerpt":"kubeadm: 主要用于提供安装kubernetes的辅助工具master: nodes: 安装kubelet, kubeadm, docker, kubectl # kubectl仅master安装即可master: kubeadm init 初始化集群主节点nodes: kubeadm join 将node节点加入集群Doc: https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.10.md 安装前配置关闭iptables 和firewalld1234$ systemctl stop iptables.service $ systemctl disable iptables.service $ systemctl stop firewalld.service $ systemctl disable firewalld.service","text":"kubeadm: 主要用于提供安装kubernetes的辅助工具master: nodes: 安装kubelet, kubeadm, docker, kubectl # kubectl仅master安装即可master: kubeadm init 初始化集群主节点nodes: kubeadm join 将node节点加入集群Doc: https://github.com/kubernetes/kubeadm/blob/master/docs/design/design_v1.10.md 安装前配置关闭iptables 和firewalld1234$ systemctl stop iptables.service $ systemctl disable iptables.service $ systemctl stop firewalld.service $ systemctl disable firewalld.service 基于主机名master和nodes主机名添加至/etc/hosts 主机网络桥接设置1234$ cat /proc/sys/net/bridge/bridge-nf-call-ip6tables 1 $ cat /proc/sys/net/bridge/bridge-nf-call-iptables 1 安装kuberbetes和docker准备镜像源到/etc/yum.repos.d/ docker-ce源 12$ cd ; wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -P /etc/yum.repos.d/ $ yum list docker-ce --showduplicates |sort -r kubernetes源 123456789101112131415$ (cat &lt;&lt; EOF [kubernetes] name=Kubernets Repo baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg EOF ) &gt; /etc/yum.repos.d/kubernetes.repo wget https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg wget https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg rpm --import yum-key.gpg rpm --import rpm-package-key.gpg yum repolist 安装kuberbetes和docker 安装kublete、kubeadm、docker、kubectl kubectl仅master安装即可 1$ yum install docker-ce-18.06.3 kubelet-1.12.0 kubeadm-1.12.0 kubectl-1.12.0 配置kuberbetes和docker docker添加代理，[service]下添加,为了翻墙更改docker默认拉取镜像的源[这一步仅仅是为了翻墙下载镜像] 123$ vim /usr/lib/systemd/system/docker.service Environment=&quot;HTTPS_PROXY=http://www.ik8s.io:10080&quot; Environment=&quot;NO_PROXY=127.0.0.0/8,172.16.0.0/16&quot; 配置开机自动启动123systemctl enable docker kubelet systemctl daemon-reload systemctl start docker 禁用Swap功能12$ vim /etc/sysconfig/kubelet KUBELET_EXTRA_ATGS=&quot;--fail-swap-on=false&quot; Master初始化12kubeadm init --help; kubeadm init --kubernetes-version=v1.12.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --ignore-preflight-errors=Swap 初始化参数 –apiserver-advertise-address：表示apiserver对外的地址是什么，默认是0.0.0.0 –apiserver-bind-port：表示apiserver的端口是什么，默认是6443 –cert-dir：加载证书的目录，默认在/etc/kubernetes/pki –config：配置文件 –ignore-preflight-errors：在预检中如果有错误可以忽略掉，比如忽略 IsPrivilegedUser,Swap.等 –kubernetes-version：指定要初始化k8s的版本信息是什么 –pod-network-cidr ：指定pod使用哪个网段，默认使用10.244.0.0/16 –service-cidr：指定service组件使用哪个网段，默认10.96.0.0/12 初始化产生信息 提示在master上执行，和加入master的命令[也可以指定–ignore-preflight-errors忽略Swap] 1234567891011121314151617 ……………………………………………………………… Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each node as root: kubeadm join 192.168.1.171:6443 --token r9105w.5r5je2vko3jyn8be --discovery-token-ca-cert-hash sha256:d7a99553b49b88d8933785fee033663adebd6d6909323cea6173d009ad66a7f8 master继续执行 123$ mkdir -p $HOME/.kube $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config $ sudo chown $(id -u):$(id -g) $HOME/.kube/config 若初始化失败：可先行下载镜像,下载脚本 12345678910$ cat pull-images.sh #!/bin/bash images=(kube-apiserver:v1.12.0 kube-controller-manager:v1.12.0 kube-scheduler:v1.12.0 kube-proxy:v1.12.0 pause:3.1 etcd:3.2.24 coredns:1.2.2) for ima in $&#123;images[@]&#125; do docker pull registry.cn-shenzhen.aliyuncs.com/lurenjia/$ima docker tag registry.cn-shenzhen.aliyuncs.com/lurenjia/$ima k8s.gcr.io/$ima docker rmi -f registry.cn-shenzhen.aliyuncs.com/lurenjia/$ima done 查看状态信息 123456789$ kubectl get cs NAME STATUS MESSAGE ERROR scheduler Healthy ok controller-manager Healthy ok etcd-0 Healthy \\&#123;&quot;health&quot;: &quot;true&quot;\\&#125; $ kubectl get nodes NAME STATUS ROLES AGE VERSION master-01 NotReady master 40m v1.12.0 master01为未就绪状态，需要一个重要的网络插件flannel 安装flannel网络组件 [参考] https://github.com/coreos/flannel 1234567891011$ kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml podsecuritypolicy.extensions/psp.flannel.unprivileged created clusterrole.rbac.authorization.k8s.io/flannel created clusterrolebinding.rbac.authorization.k8s.io/flannel created serviceaccount/flannel created configmap/kube-flannel-cfg created daemonset.extensions/kube-flannel-ds-amd64 created daemonset.extensions/kube-flannel-ds-arm64 created daemonset.extensions/kube-flannel-ds-arm created daemonset.extensions/kube-flannel-ds-ppc64le created daemonset.extensions/kube-flannel-ds-s390x created kubectl get -h 命令 注释 kubectl get cs 查看状态信息 cs=componentstatus kubectl get nodes 查看node信息 kubectl get pods [-n kube-system -o wide] 查看所有pod,-n 指定命名空间,-o wide 输出扩展信息 kubectl get ns 查看所有命名空间","categories":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/categories/kubernetes/"}],"tags":[{"name":"kubernetes","slug":"kubernetes","permalink":"https://ngames-dev.cn/tags/kubernetes/"}]},{"title":"Homebrew源替换及重置","slug":"Homebrew源替换及重置","date":"2019-03-06T16:00:00.000Z","updated":"2019-09-06T09:23:30.119Z","comments":true,"path":"2019/03/07/Homebrew源替换及重置/","link":"","permalink":"https://ngames-dev.cn/2019/03/07/Homebrew源替换及重置/","excerpt":"替换123456789- 替换brew.git:cd &quot;$(brew --repo)&quot;git remote set-url origin https://mirrors.ustc.edu.cn/brew.git- 替换homebrew-core.git:cd &quot;$(brew --repo)/Library/Taps/homebrew/homebrew-core&quot;git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.gitbrew update","text":"替换123456789- 替换brew.git:cd &quot;$(brew --repo)&quot;git remote set-url origin https://mirrors.ustc.edu.cn/brew.git- 替换homebrew-core.git:cd &quot;$(brew --repo)/Library/Taps/homebrew/homebrew-core&quot;git remote set-url origin https://mirrors.ustc.edu.cn/homebrew-core.gitbrew update 重置123456789- 重置brew.git:cd &quot;$(brew --repo)&quot;git remote set-url origin https://github.com/Homebrew/brew.git- 重置homebrew-core.git:cd &quot;$(brew --repo)/Library/Taps/homebrew/homebrew-core&quot;git remote set-url origin https://github.com/Homebrew/homebrew-core.gitbrew update","categories":[{"name":"Homebrew","slug":"Homebrew","permalink":"https://ngames-dev.cn/categories/Homebrew/"}],"tags":[{"name":"Homebrew","slug":"Homebrew","permalink":"https://ngames-dev.cn/tags/Homebrew/"},{"name":"源","slug":"源","permalink":"https://ngames-dev.cn/tags/源/"}]},{"title":"kafka docker-compose","slug":"kafka","date":"2019-03-06T16:00:00.000Z","updated":"2019-09-06T09:23:30.119Z","comments":true,"path":"2019/03/07/kafka/","link":"","permalink":"https://ngames-dev.cn/2019/03/07/kafka/","excerpt":"单实例 docker-compose启动 123456789101112131415161718192021222324version: '2'services: zoo1: image: wurstmeister/zookeeper restart: unless-stopped hostname: zoo1 ports: - \"2181:2181\" container_name: zookeeper kafka1: image: wurstmeister/kafka ports: - \"9092:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: localhost KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\" KAFKA_BROKER_ID: 1 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_CREATE_TOPICS: \"stream-in:1:1,stream-out:1:1\" depends_on: - zoo1 container_name: kafka","text":"单实例 docker-compose启动 123456789101112131415161718192021222324version: '2'services: zoo1: image: wurstmeister/zookeeper restart: unless-stopped hostname: zoo1 ports: - \"2181:2181\" container_name: zookeeper kafka1: image: wurstmeister/kafka ports: - \"9092:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: localhost KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\" KAFKA_BROKER_ID: 1 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_CREATE_TOPICS: \"stream-in:1:1,stream-out:1:1\" depends_on: - zoo1 container_name: kafka kafka集群 docker-compose启动 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162version: '3'services: zoo1: image: wurstmeister/zookeeper restart: unless-stopped hostname: zoo1 ports: - \"2181:2181\" container_name: zookeeper kafka1: image: wurstmeister/kafka ports: - \"9091:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: 172.16.149.242 # 此处填写为宿主机IP KAFKA_ADVERTISED_PORT: 9091 KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\" KAFKA_BROKER_ID: 1 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 KAFKA_CREATE_TOPICS: \"stream-in:1:1,stream-out:1:1\" depends_on: - zoo1 container_name: kafka1 kafka2: image: wurstmeister/kafka ports: - \"9092:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: 172.16.149.242 KAFKA_ADVERTISED_PORT: 9092 KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\" KAFKA_BROKER_ID: 2 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 depends_on: - zoo1 container_name: kafka2 kafka3: image: wurstmeister/kafka ports: - \"9093:9092\" environment: KAFKA_ADVERTISED_HOST_NAME: 172.16.149.242 KAFKA_ADVERTISED_PORT: 9093 KAFKA_ZOOKEEPER_CONNECT: \"zoo1:2181\" KAFKA_BROKER_ID: 3 KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1 depends_on: - zoo1 container_name: kafka3 kafka-manage: image: sheepkiller/kafka-manager ports: - \"9999:9000\" environment: ZK_HOSTS: zoo1:2181 APPLICATION_SECRET: \"letmein\" container_name: kafka-manager depends_on: - zoo1 测试脚本 生产者 kafka-send.py 123456789101112131415161718192021222324# -*- coding: utf-8 -*-import jsonfrom kafka import KafkaProducerfor i in range(500): msg_dict = &#123; \"sleep_time\": 10, \"db_config\": &#123; \"database\": \"xx_db\", \"host\": \"xxxx\" &#125;, \"table\": \"msg\", \"msg\": \"Hello World\" &#125; msg_dict[\"number\"] = i msg=json.dumps(msg_dict) # print(msg) for host in [\"172.16.149.242:9091\",\"172.16.149.242:9092\",\"172.16.149.242:9093\"]: producer = KafkaProducer(bootstrap_servers=host) producer.send('sakura', value=msg.encode(\"utf-8\"), partition=0)producer.close() 消费者 kafka-consumer.py 1234567# -*- coding: utf-8 -*-from kafka import KafkaConsumerconsumer = KafkaConsumer('sakura', bootstrap_servers=[\"172.16.149.242:9092\"])for msg in consumer: recv = \"%s:%d:%d: key=%s value=%s\" %(msg.topic, msg.partition, msg.offset, msg.key, msg.value) print(recv)","categories":[{"name":"kafka","slug":"kafka","permalink":"https://ngames-dev.cn/categories/kafka/"}],"tags":[{"name":"docker compose","slug":"docker-compose","permalink":"https://ngames-dev.cn/tags/docker-compose/"},{"name":"kafka","slug":"kafka","permalink":"https://ngames-dev.cn/tags/kafka/"}]},{"title":"etcd-Group","slug":"etcd-Group","date":"2019-02-28T16:00:00.000Z","updated":"2019-10-14T07:39:31.003Z","comments":true,"path":"2019/03/01/etcd-Group/","link":"","permalink":"https://ngames-dev.cn/2019/03/01/etcd-Group/","excerpt":"docker-compose方式部署etcd集群","text":"docker-compose方式部署etcd集群 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102version: '3'services: etcd1: container_name: etcd1 image: registry.cn-hangzhou.aliyuncs.com/coreos_etcd/etcd:v3 ports: - \"12379:2379\" - \"14001:4001\" - \"12380:2380\" environment: - TZ=CST-8 - LANG=zh_CN.UTF-8 command: /usr/local/bin/etcd -name etcd1 -data-dir /etcd-data -advertise-client-urls http://172.33.0.11:2379,http://172.33.0.11:4001 -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 -initial-advertise-peer-urls http://172.33.0.11:2380 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token docker-etcd -initial-cluster etcd1=http://172.33.0.11:2380,etcd2=http://172.33.0.22:2380,etcd3=http://172.33.0.33:2380 -initial-cluster-state new volumes: - \"/yibao/etcd/data1:/etcd-data\" networks: test_net: ipv4_address: 172.33.0.11 labels: - project.source= - project.extra=public-image - project.depends= - project.owner=LHZ etcd2: container_name: etcd2 image: registry.cn-hangzhou.aliyuncs.com/coreos_etcd/etcd:v3 ports: - \"22379:2379\" - \"24001:4001\" - \"22380:2380\" environment: - TZ=CST-8 - LANG=zh_CN.UTF-8 command: /usr/local/bin/etcd -name etcd2 -data-dir /etcd-data -advertise-client-urls http://172.33.0.22:2379,http://172.33.0.22:4001 -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 -initial-advertise-peer-urls http://172.33.0.22:2380 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token docker-etcd -initial-cluster etcd1=http://172.33.0.11:2380,etcd2=http://172.33.0.22:2380,etcd3=http://172.33.0.33:2380 -initial-cluster-state new volumes: - \"/yibao/etcd/data2:/etcd-data\" networks: test_net: ipv4_address: 172.33.0.22 labels: - project.source= - project.extra=public-image - project.depends= - project.owner=LHZ etcd3: container_name: etcd3 image: registry.cn-hangzhou.aliyuncs.com/coreos_etcd/etcd:v3 ports: - \"32379:2379\" - \"34001:4001\" - \"32380:2380\" environment: - TZ=CST-8 - LANG=zh_CN.UTF-8 command: /usr/local/bin/etcd -name etcd3 -data-dir /etcd-data -advertise-client-urls http://172.33.0.33:2379,http://172.33.0.33:4001 -listen-client-urls http://0.0.0.0:2379,http://0.0.0.0:4001 -initial-advertise-peer-urls http://172.33.0.33:2380 -listen-peer-urls http://0.0.0.0:2380 -initial-cluster-token docker-etcd -initial-cluster etcd1=http://172.33.0.11:2380,etcd2=http://172.33.0.22:2380,etcd3=http://172.33.0.33:2380 -initial-cluster-state new volumes: - \"/yibao/etcd/data3:/etcd-data\" networks: test_net: ipv4_address: 172.33.0.33 labels: - project.source= - project.extra=public-image - project.depends= - project.owner=LHZnetworks: test_net: driver: bridge ipam: driver: default config: - subnet: 172.33.0.0/24","categories":[{"name":"etcd","slug":"etcd","permalink":"https://ngames-dev.cn/categories/etcd/"}],"tags":[{"name":"etcd","slug":"etcd","permalink":"https://ngames-dev.cn/tags/etcd/"},{"name":"docker-compose","slug":"docker-compose","permalink":"https://ngames-dev.cn/tags/docker-compose/"}]},{"title":"docker-registry","slug":"docker-registry","date":"2019-02-26T16:00:00.000Z","updated":"2019-10-14T07:39:31.002Z","comments":true,"path":"2019/02/27/docker-registry/","link":"","permalink":"https://ngames-dev.cn/2019/02/27/docker-registry/","excerpt":"安装registry1234567#!/bin/bashdocker run -d \\ -p 5001:5000 \\ --restart=always \\ --name registry \\ -v $PWD/docker:/var/lib/registry \\ registry:2","text":"安装registry1234567#!/bin/bashdocker run -d \\ -p 5001:5000 \\ --restart=always \\ --name registry \\ -v $PWD/docker:/var/lib/registry \\ registry:2 配置nginx代理，带密码验证123456789101112131415161718192021222324252627282930/etc/nginx/conf.d/docker-registry-htpasswdhtpasswd -c docker-registry-htpasswd usernamecat registry.confupstream docker-registry &#123; server 172.16.149.242:5001;&#125;server &#123; listen 80; #listen 443; server_name registry.ngames-dev.cn; add_header &apos;Docker-Distribution-Api-Version&apos; &apos;registry/2.0&apos; always; #ssl on; #ssl_certificate /etc/nginx/conf.d/server.crt; #ssl_certificate_key /etc/nginx/conf.d/server.key; location / &#123; auth_basic &quot;Please Input username/password&quot;; auth_basic_user_file &quot;/etc/nginx/conf.d/docker-registry-htpasswd&quot;; proxy_pass http://docker-registry; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarder-Porto $scheme; proxy_read_timeout 600; client_max_body_size 0; &#125;&#125; 去https 验证登陆仓库 1234docker login registry.ngames-dev.cnUsername: sakuraPassword: Error response from daemon: Get https://registry.ngames-dev.cn/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers) 从docker1.3.2版本开始默认docker registry使用的是https 12345678cat /etc/docker/daemon.json &#123; &quot;insecure-registries&quot;: [&quot;registry.ngames-dev.cn&quot;]&#125;systemctl daemon-reloadsystemctl restart docker 之后再次登陆，成功后会在$HOME/.docker/config.json中auths添加一串登陆记录 12345678910111213&#123; \"auths\": &#123; \"https://index.docker.io/v1/\": &#123; \"auth\": \"c2FrdXJhZ2FhcmE6R3VvanVuNTIzMDA=\" &#125;, \"registry.ngames-dev.cn\": &#123; \"auth\": \"c2FrdXJhOjUyMzAwMxxxxxxxxxxxxxxxx\" &#125; &#125;, \"HttpHeaders\": &#123; \"User-Agent\": \"Docker-Client/18.09.2 (linux)\" &#125;&#125; 上传镜像 镜像更改名字 上传镜像 12docker tag alpine:latest registry.ngames-dev.cn/sakura/alpine:v1docker push registry.ngames-dev.cn/sakura/alpine:v1 查看镜像 12curl -XGET username:password@registry.ngames-dev.cn/v2/_catalog&#123;&quot;repositories&quot;:[&quot;sakura/alpine&quot;]&#125; 查看镜像标签信息 12curl -XGET username:password@registry.ngames-dev.cn/v2/sakura/alpine/tags/list&#123;&quot;name&quot;:&quot;sakura/alpine&quot;,&quot;tags&quot;:[&quot;v1&quot;]&#125; 删除镜像 1,获取Docker-Content-Digest 12345678910111213curl --header &quot;Accept: application/vnd.docker.distribution.manifest.v2+json&quot; -I username:password@registry.ngames-dev.cn/v2/sakura/alpine/manifests/v1HTTP/1.1 200 OKServer: nginx/1.15.8Date: Sun, 03 Mar 2019 02:32:15 GMTContent-Type: application/vnd.docker.distribution.manifest.v2+jsonContent-Length: 528Connection: keep-aliveDocker-Content-Digest: sha256:25b4d910f4b76a63a3b45d0f69a57c34157500faf6087236581eca221c62d214Docker-Distribution-Api-Version: registry/2.0Etag: &quot;sha256:25b4d910f4b76a63a3b45d0f69a57c34157500faf6087236581eca221c62d214&quot;X-Content-Type-Options: nosniffDocker-Distribution-Api-Version: registry/2.0 2,删除 1curl -XDELETE username:password@registry.ngames-dev.cn/v2/sakura/alpine/manifests/sha256:25b4d910f4b76a63a3b45d0f69a57c34157500faf6087236581eca221c62d214","categories":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/tags/docker/"}]},{"title":"docker-compose模板文件主要命令","slug":"docker-compose模板文件主要命令","date":"2019-02-19T16:00:00.000Z","updated":"2019-09-06T09:23:30.118Z","comments":true,"path":"2019/02/20/docker-compose模板文件主要命令/","link":"","permalink":"https://ngames-dev.cn/2019/02/20/docker-compose模板文件主要命令/","excerpt":"docker-compose主要命令及功能","text":"docker-compose主要命令及功能 命令 功能 build 指定Dockerfile所在文件的路径 cap_add, cap_drop 指定容器的内核能力capacity分配 command 覆盖容器启动后默认执行的命令 cgroup_parent 指定cgroup组 container_name 指定容器名称 devices 指定设备映射关系 depends_on 指定国歌服务之间依赖关系 dns 自定义DNS服务器 dns_search 配置DNS搜索域 dockerfile 指定额外的编译镜像Dockerfile文件 entrypoint 覆盖容器中默认的入口命令 env_file 从文件中获取环境变量 environment 设置环境变量 expose 暴露端口，但不映射到宿主机，只被链接的服务访问 extends 基于其他模板文件进行扩展 external_links 链接到docker-compose.yml外部的容器 exter_hosts 指定额外的host名称映射信息 healthcheck 指定检测应用健康状态的机制 image 指定镜像名称或镜像ID isolation 配置容器隔离机制 labels 为容器添加Dockers元数据信息 links 链接到其他服务器中的容器（旧用法，被移除） logging 跟日志相关的配置 network_mode 设置网络模式 networks 所加入的网络 pid 跟宿主机系统共享进程命名空间 ports 暴露端口信息 secrets 配置应用的秘密数据 security_opt 指定容器模板标签label机制的默认属性【用户，角色，类型，级别等】 stop_grace_period 指定应用停止是，容器的优雅停止期限。过期通过SIGKILL强制退出.默认10s stop_signal 指定停止容器的信号，默认为SIGTERM sysctls 配置容器内核参数 ulimits 配置容器的ulimits限制值 userns_mode 指定用户命名空间模式 volumes 数据卷所挂载路径设置 restart 指定重启策略 deploy 指定部署和运行时容器相关配置，命令只在Swarm模式下生效，且只支持docker stack deploy命令部署","categories":[{"name":"docker compose","slug":"docker-compose","permalink":"https://ngames-dev.cn/categories/docker-compose/"}],"tags":[{"name":"docker compose","slug":"docker-compose","permalink":"https://ngames-dev.cn/tags/docker-compose/"}]},{"title":"docker-compose样例","slug":"docker-compose样例","date":"2019-02-16T16:00:00.000Z","updated":"2019-10-14T07:39:31.001Z","comments":true,"path":"2019/02/17/docker-compose样例/","link":"","permalink":"https://ngames-dev.cn/2019/02/17/docker-compose样例/","excerpt":"","text":"networks 创建新的网络 1docker network create --driver=bridge --subnet=172.33.0.0/24 test_net 对比 docker-compose方式 1234567networks: test_net: driver: bridge ipam: driver: default config: - subnet: 172.33.0.0/24 lnmp nginx/php site.conf 1234567891011121314151617181920212223242526272829303132version: '3'services: nginx: image: nginx container_name: lnmp-nginx depends_on: - php ports: - \"5008:80\" networks: - \"test_net\" volumes: - ./conf.d/site.conf:/etc/nginx/conf.d/default.conf - ./www:/www links: - php:php php: image: php:5.6-fpm container_name: lnmp-php expose: - 9000 networks: - \"test_net\" volumes: - \"./www:/www\"networks: test_net: driver: bridge ipam: driver: default config: - subnet: 172.32.0.0/24 mysql1docker run -p 3002:3306 --name mysql3302 -v $PWD/conf:/etc/mysql/conf.d -v $PWD/logs:/logs -v $PWD/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.6 对比 docker-compose 1234567891011121314151617181920212223242526272829303132333435363738394041424344version: \"3\"services: mysql1: image: mysql:5.6 volumes: - ./data1:/var/lib/mysql - ./conf1:/etc/mysql - ./logs1:/logs container_name: mysql1 ports: - 3303:3306 environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123456 command: --character-set-server=utf8mb4 --max_allowed_packet=32M networks: - mysql_net mysql2: image: mysql:5.6 volumes: - ./data2:/var/lib/mysql - ./conf2:/etc/mysql - ./logs2:/logs container_name: mysql2 ports: - 3304:3306 environment: TZ: Asia/Shanghai MYSQL_ROOT_PASSWORD: 123456 command: --character-set-server=utf8mb4 --max_allowed_packet=32M networks: - mysql_netnetworks: mysql_net: driver: bridge ipam: config: - subnet: 172.55.0.0/24 build Dockerfile-app 1234567891011FROM alpine:latestRUN echo -e &quot;http://mirrors.ustc.edu.cn/alpine/v3.7/main\\n\\http://mirrors.ustc.edu.cn/alpine/v3.7/community&quot; &gt; /etc/apk/repositoriesRUN apk --update add curl bash openjdk8-jre-base &amp;&amp; \\ rm -rf /var/cache/apk/*ENV JAVA_HOME /usr/lib/jvm/default-jvmENV PATH $&#123;PATH&#125;:$&#123;JAVA_HOME&#125;/bin docker-compose-java.yml 123456version: '3'services: app: build: context: /data/alpine-java dockerfile: Dockerfile-java 运行 docker-compose -f docker-compose-java.yml build","categories":[{"name":"docker compose","slug":"docker-compose","permalink":"https://ngames-dev.cn/categories/docker-compose/"}],"tags":[{"name":"docker compose","slug":"docker-compose","permalink":"https://ngames-dev.cn/tags/docker-compose/"}]},{"title":"Docker网络","slug":"Docker网络","date":"2019-02-14T16:00:00.000Z","updated":"2019-10-14T07:39:31.000Z","comments":true,"path":"2019/02/15/Docker网络/","link":"","permalink":"https://ngames-dev.cn/2019/02/15/Docker网络/","excerpt":"mount 命令查看挂载信息 /etc/hosts, /etc/hostname, /etc/resolve.conf 是临时的，只是在运行容器时保留，docker commit不会被提交 12345root@gitlab:/# mount |grep &quot;/etc&quot; /dev/vdb1 on /etc/gitlab type ext4 (rw,relatime,data=ordered)/dev/vda1 on /etc/resolv.conf type ext4 (rw,relatime,data=ordered) /dev/vda1 on /etc/hostname type ext4 (rw,relatime,data=ordered) /dev/vda1 on /etc/hosts type ext4 (rw,relatime,data=ordered)","text":"mount 命令查看挂载信息 /etc/hosts, /etc/hostname, /etc/resolve.conf 是临时的，只是在运行容器时保留，docker commit不会被提交 12345root@gitlab:/# mount |grep &quot;/etc&quot; /dev/vdb1 on /etc/gitlab type ext4 (rw,relatime,data=ordered)/dev/vda1 on /etc/resolv.conf type ext4 (rw,relatime,data=ordered) /dev/vda1 on /etc/hostname type ext4 (rw,relatime,data=ordered) /dev/vda1 on /etc/hosts type ext4 (rw,relatime,data=ordered) 检查容器访问外部网络 在宿主机Linux中检测转发是否打开 123$ sudo sysctl net.ipv4.ip_forwardnet.ipv4.ip_forward = 1$ sudo sysctl -w net.ipv4.ip_forward=1 # 打开 查看宿主机nat表上POSTROUTING链规则1iptables -t nat -nvL POSTROUTING Docker自定义网桥 默认docker0网桥，在Docker服务启动时可以使用-b BRIDGE 来指定使用的网桥 1234567891011121314- 停止docker服务,删除docker0网桥$ systemctl stop docker$ ip link set dev docker0 down$ brctl delbr docker0- 创建一个网桥bridge0$ brctl addbr bridge0$ ip addr add 192.168.5.1/24 dev bridge0$ ip link set dev bridge0 up- 查看网桥并启动$ ip addr show bridge0$ echo &apos;DOCKER_OPTS=&quot;-b=bridge0&quot;&apos; &gt;&gt; /etc/defalut/docker$ sudo systemctl start docker","categories":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/tags/docker/"}]},{"title":"Docker_Registry","slug":"Docker_Registry","date":"2019-02-13T16:00:00.000Z","updated":"2019-10-14T07:39:31.000Z","comments":true,"path":"2019/02/14/Docker_Registry/","link":"","permalink":"https://ngames-dev.cn/2019/02/14/Docker_Registry/","excerpt":"Docker registry1docker run -d -p 5001:5000 --restart=always --name registry -v /data/registry:/var/lib/registry registry:2","text":"Docker registry1docker run -d -p 5001:5000 --restart=always --name registry -v /data/registry:/var/lib/registry registry:2 Nginx 代理 123456789101112131415161718192021222324$ cat registry.confupstream docker-registry &#123; server 172.16.149.242:5001;&#125;server &#123; listen 80; server_name registry.xxxxxxx.cn; add_header &apos;Docker-Distribution-Api-Version&apos; &apos;registry/2.0&apos; always; location / &#123; auth_basic &quot;Please Input username/password&quot;; auth_basic_user_file &quot;/etc/nginx/conf.d/docker-registry-htpasswd&quot;; proxy_pass http://docker-registry; proxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarder-Porto $scheme; proxy_read_timeout 600; client_max_body_size 0; &#125;&#125; docker-registry-htpasswd nginx认证 1htpasswd -c /etc/nginx/conf.d/docker-registry-htpasswd $username harbor harbor是一个企业级的 Docker Registry，可以实现 images 的私有存储和日志统计权限控制等功能，并支持创建多项目(Harbor 提出的概念)，基于官方 Registry V2 实现 下载安装地址https://github.com/goharbor/harborhttps://github.com/goharbor/harbor/releases","categories":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/tags/docker/"},{"name":"harbor","slug":"harbor","permalink":"https://ngames-dev.cn/tags/harbor/"}]},{"title":"Docker-Ubuntu笔记","slug":"Docker-Ubuntu笔记","date":"2019-02-12T16:00:00.000Z","updated":"2019-09-06T09:23:30.116Z","comments":true,"path":"2019/02/13/Docker-Ubuntu笔记/","link":"","permalink":"https://ngames-dev.cn/2019/02/13/Docker-Ubuntu笔记/","excerpt":"Docker ubuntu 笔记Docker安装Ubuntu sshd服务启动容器,进入容器1docker run -it --name ubuntu_v1 ubuntu bash ununtu配置软件源1234567891011121314apt-get updatevi /etc/apt/soutces.list.d/163.listdeb http://mirrors.163.com/ubuntu/ wily main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-backports main restricted universe multiverseapt-get update","text":"Docker ubuntu 笔记Docker安装Ubuntu sshd服务启动容器,进入容器1docker run -it --name ubuntu_v1 ubuntu bash ununtu配置软件源1234567891011121314apt-get updatevi /etc/apt/soutces.list.d/163.listdeb http://mirrors.163.com/ubuntu/ wily main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-security main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-updates main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-proposed main restricted universe multiversedeb http://mirrors.163.com/ubuntu/ wily-backports main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-security main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-updates main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-proposed main restricted universe multiversedeb-src http://mirrors.163.com/ubuntu/ wily-backports main restricted universe multiverseapt-get update 安装配置启动ssh服务1234agt-get install openssh-servermkdir -p /var/run/sshd/usr/sbin/sshd -D &amp;sed -ri 's/session required pam_loginuid.so/#session required pam_loginuid.so/g' /etc/pam.d/sshd 将Docker Container保存为Image创建run-ssh.sh脚本 12#!/bin/bash/usr/sbin/sshd -D 123docker commit ubuntu_v1 sshd:ubuntu启动新的sshd镜像docker run -d -p 10022:22 sshd:ubuntu /run.sh","categories":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/tags/docker/"},{"name":"ubuntu","slug":"ubuntu","permalink":"https://ngames-dev.cn/tags/ubuntu/"}]},{"title":"Docker常用镜像","slug":"Docker常用镜像","date":"2019-02-12T16:00:00.000Z","updated":"2019-10-14T07:39:30.999Z","comments":true,"path":"2019/02/13/Docker常用镜像/","link":"","permalink":"https://ngames-dev.cn/2019/02/13/Docker常用镜像/","excerpt":"BusyBox1docker search busybox https://hub.docker.com/_/busybox Alpine 大小只有5M,大部分Docker官方镜像都支持Alpine作为基础镜像，很容易迁移 ubuntu/debian -&gt; alpine python:2.7 -&gt; python:3.6-alpine ruby:2.6 -&gt; ruby:2.6-alpine 更改时间","text":"BusyBox1docker search busybox https://hub.docker.com/_/busybox Alpine 大小只有5M,大部分Docker官方镜像都支持Alpine作为基础镜像，很容易迁移 ubuntu/debian -&gt; alpine python:2.7 -&gt; python:3.6-alpine ruby:2.6 -&gt; ruby:2.6-alpine 更改时间 123apk update &amp;&amp; apk add tzdataln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeecho &quot;Asia/Shanghai&quot; &gt; /etc/timezone 安装包可以使用apk工具: 1apk add --no-cache &lt;package&gt; 阿里镜像 1sed -i &quot;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g&quot; /etc/apk/repositories 科大镜像 1sed -i &quot;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&quot; /etc/apk/repositories http://alpinelinyx.org https://hub.docker.com/_/alpine/ Debian和Ubuntu Debain和ubuntu非常适合开发使用 Debain配置utf-8支持 1234docker run -it debian bashapt-get update &amp;&amp; apt-get install -y locales &amp;&amp; rm -rf /var/lib/apt/lists/* \\ localedef -i en_US -c -f UTF-8 -A /usr/share/locale/locale.alias en_US.UTF-8 Debain更改时区，时间 123apt-get update \\ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\/usr/sbin/ntpdate time.cuit.edu.cn Ubuntu更改时区，时间 1apt-get update &amp;&amp; apt-get install -y tzdata 选择时区 docker中，可 12RUN cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\echo &apos;Asia/Shanghai&apos; &gt;/etc/timezone Centos 设置时区12ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\echo &apos;Asia/Shanghai&apos; &gt;/etc/timezone MongoDB1docker run -d --name docker_mongodb -p 27017:27017 -v /data/mongo/db:/data/db mongo MySQL123456docker run -d -p 3002:3306 --name mysql56 \\ -v $PWD/conf:/etc/mysql/conf.d \\ -v $PWD/logs:/logs \\ -v $PWD/data:/var/lib/mysql \\ -e MYSQL_ROOT_PASSWORD=123456 \\ mysql:5.6 Redis123docker run --name redis-test -p 6379:6379 -d \\ --restart=always redis:latest redis-server \\ --appendonly yes --requirepass &quot;your passwd&quot; -p 6379:6379 :将容器内端口映射到宿主机端口(右边映射到左边) redis-server –appendonly yes : 在容器执行redis-server启动命令，并打开redis持久化配置 requirepass “your passwd” :设置认证密码 –restart=always : 随docker启动而启动","categories":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/categories/docker/"}],"tags":[{"name":"docker","slug":"docker","permalink":"https://ngames-dev.cn/tags/docker/"}]}]}